\documentclass[12pt,a4paper,leqno]{report}

\usepackage[ansinew]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsthm}
\usepackage{amsfonts}         
\usepackage{amsmath}
\usepackage{amssymb}

\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\No}{\mathbb{N}_0}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\diam}{\operatorname{diam}}

\newcommand{\eps}{\varepsilon}

\theoremstyle{plain}
\newtheorem{theo}[equation]{Theorem}
\newtheorem{lem}[equation]{Lemma}
\newtheorem{prop}[equation]{Proposition}
\newtheorem{cor}[equation]{Corollary}

\theoremstyle{definition}
\newtheorem{defi}[equation]{Definition}
\newtheorem{conjec}[equation]{Conjecture}
\newtheorem{exa}[equation]{Example}

\theoremstyle{remark}
\newtheorem{rema}[equation]{Remark}

\pagestyle{plain}
\setcounter{page}{1}
\addtolength{\hoffset}{-1.15cm}
\addtolength{\textwidth}{2.3cm}
\addtolength{\voffset}{0.45cm}
\addtolength{\textheight}{-0.9cm}

\title{Random matrix stuff}
\author{Olli Hirviniemi}
\date{}

\begin{document}

\maketitle

\tableofcontents

\chapter{Introduction}\label{intro}

The purpose of this thesis is to make me a master.


Something something about random variables and stuff, matrices also. Spectral theorem might be good to state here?

\begin{theo}\label{specth}
\emph{Spectral theorem.} An $n \times n$ hermitian matrix has $n$ real eigenvalues, counting multiplicities.
\end{theo}

\begin{proof}
See \cite{Tao}.
\end{proof}

\chapter{Setting}\label{setup}

\begin{defi}
Suppose that $(\xi_{ij})_{i,j \in \Z_+}$ are random variables with $(\xi_{ij})_{i \leq j}$ being independent, $(\xi_{ii})_{i \in \Z_+}$ identically distributed real random variables, $(\xi_{ij})_{i<j}$ identically distributed complex random variables, and $\xi_{ij}=\overline{\xi_{ji}}$ for $i>j$. With such random variables, an associated \emph{Wigner matrix} (of size $n$) is the hermitian matrix
\begin{equation*}
W_n = (\xi_{ij})_{1\leq i,j \leq n}.
\end{equation*} 
\end{defi}

Assume that every $\xi_{ij}$ has mean $0$ and unit variance. We will also assume that the fourth moment of the off-diagonal elements is bounded by some constant $K$: this is true in many important special cases such as Gaussian matrix ensembles.

Since $W_n$ is hermitian, it has $n$ real eigenvalues by spectral theorem.
Trying to understand how these eigenvalues are distributed is the purpose of this section: namely, deriving the \emph{semicircular law}.

First consider the magnitude of the eigenvalues. A simple example of $n \times n$ -matrix of ones shows that even with bounded coefficients, the eigenvalues can grow in magnitude as the size of the matrix increases. So there is need to normalize the matrix somehow.

Following lemma motivates the normalizing factor used:

\begin{lem}
Let $x$ be a unit vector of $\C^n$. Then for any $\lambda > 0$, 
\begin{equation*}
P(|W_nx|>\sqrt{n}\lambda) \leq \frac{8}{\lambda^2}.
\end{equation*}
\end{lem}

\begin{proof}
Fix a unit vector $x$. Split $W_n = M_1 + M_2$, where $M_1$ consist of upper triangular elements of $W_n$ (so $M_1$ is zero below the main diagonal).
If we denote the components of $x$ with $x_i$ and rows of $M_1$ by $R_i$, then we get following upper bound using independence and zero mean hypothesis:
\begin{eqnarray*}
E(|M_1x|^2) & = &  
E(\sum_{i=1}^n |R_i \cdot x|^2) \\
& = & \sum_{i=1}^n E(|R_i \cdot x|^2)\\
& = & \sum_{i=1}^n 
E(|\sum_{j=i}^n \xi_{ij} x_j|^2)\\
& = & \sum_{i=1}^n 
E(\sum_{j=i}^n \sum_{k=i}^n \xi_{ij} x_j\overline{\xi_{ik} x_k}) \\
& = & \sum_{i=1}^n 
\sum_{j=i}^n \sum_{k=i}^n E(\xi_{ij} x_j\overline{\xi_{ik} x_k}) \\
& = & \sum_{i=1}^n 
\sum_{j=i}^n |x_j|^2E(|\xi_{ij}|^2)\\
& = & \sum_{i=1}^n \sum_{j=i}^n |x_j|^2\\
& \leq & \sum_{i=1}^n \sum_{j=1}^n |x_j|^2\\
& = & \sum_{i=1}^n 1\\
& = & n
\end{eqnarray*}
Now by Markov's inequality 
\begin{equation*}
P(|M_1x|^2 \geq \lambda^2) \leq  \frac{n}{\lambda^2},
\end{equation*}
or equivalently
\begin{equation*}
P(|M_1x| \geq \sqrt{n} \lambda) \leq  \frac{1}{\lambda^2}.
\end{equation*}
Same argument also applies for $M_2$ with summing $j$ and $k$ from $1$ to $i-1$, and we have
\begin{equation*}
P(|M_2x| \geq \sqrt{n} \lambda) \leq  \frac{1}{\lambda^2}. 
\end{equation*}
Using triangle inequality, we can estimate
\begin{eqnarray*}
P(|W_nx| \geq \sqrt{n} \lambda) & \leq & P(|M_1x|+|M_2x| \geq \sqrt{n} \lambda)\\
& \leq & P(|M_1x| \geq \sqrt{n} \frac{\lambda}{2}) + P(|M_2x| \geq \sqrt{n} \frac{\lambda}{2})\\
& \leq & \frac{4}{\lambda^2} + \frac{4}{\lambda^2}\\
& = & \frac{8}{\lambda^2}.
\end{eqnarray*}
\end{proof} 


Motivated by the previous lemma, consider now matrices $\frac{1}{\sqrt{n}}W_n$. Looking at the distribution of the eigenvalues, some kind of limit behaviour seems to apply. To handle this rigorously a precise definition for convergence is necessary.

\begin{defi}
Given a hermitian matrix $A$ with eigenvalues $\lambda_1, \ldots , \lambda_n$, the empirical spectral distribution of $A$ is a probability measure given by
\begin{equation*}
\mu_A := \frac{1}{n} \sum_{i=1}^n \delta_{\lambda_i},
\end{equation*}
where $\delta_x$ is a Dirac measure for $x \in \R$.
\end{defi}

Recall that a sequence of measures $\mu_n$ is said to converge to $\mu$ in vague topology if for every compactly supported continuous function $\phi$, $\int_{\R} \phi d\mu_n$ converges to $\int_{\R}\phi d\mu$. When $A$ is a random hermitian matrix, $\mu_A$ is a random probability measure.

\begin{defi}
Let $(\mu_n)_{n=1}^{\infty}$ be a sequence of random measures.
If for any test function $\phi$, the sequence $\int_{\R} \phi d\mu_n$ converges to $\int_{\R} \phi d\mu$ almost certainly, the we say that $\mu_n$ converges to $\mu$ almost certainly.
\end{defi}

\chapter{Stieltjes transform}\label{stieltjes}


It is possible to obtain the semicircular law in different ways, for example by considering the moments of $\mu_{\frac{1}{\sqrt{n}}W_n}$. We will return to the moments later. Now define the following tool, the Stieltjes transform:

\begin{defi}
For a probability measure $\mu$ on real numbers, define its \emph{Stieltjes transform} $S_{\mu}$ to be a complex function defined in upper half plane (and thus outside the support of $\mu$) as
\begin{equation*}
S_{\mu} (z) := \int_{\R} \frac{1}{x-z} d \mu(x).
\end{equation*}
\end{defi}

Some basic properties:

Analytic, with derivative bounded by blah

The following two theorems demonstrate why the Stieltjes transform is helpful when investigating the limit behaviour of probability measures. The first allows recovering the probability measure from its Stieltjes transform and the second links the convergence of the measures to the convergence of their Stieltjes transforms.

\begin{theo}
Let $\mu$ be a probability measure on the real line, and $f_b : \R -> \R$ be the imaginary part of the function $a  \to \frac{1}{\pi} S_{\mu}(a+ib)$.  For $\phi$, we have
\begin{equation*}
\lim_{b \to 0} \int_{\R} \phi(x) f_b (x) dx = \int_{\R} \phi(x) d\mu(x).
\end{equation*}
\end{theo}

\begin{proof}
The imaginary part of the integral kernel in Stieltjes transform for $z=a+ib$ is
\begin{equation*}
\frac{b}{(x-a)^2+b^2}.
\end{equation*}
This means that

\begin{eqnarray*}
\lim_{b \to 0} \int_{\R} \phi(x) f_b (x) dx & = & \lim_{b \to 0} \int_{\R} \phi(x) \int_{\R} \frac{b}{(y-x)^2+b^2} d\mu(y) dx\\
& = & \lim_{b \to 0} \int_{\R} \int_{\R} \frac{b}{(y-x)^2+b^2} \phi(x) dx d\mu(y)\\
& = & \int_{\R} \phi(y) d\mu(y).
\end{eqnarray*}
The last equality holds because the kernels $\frac{b}{\pi((x-a)^2+b^2)}$ are an approximation of identity.
\end{proof}

\begin{theo}
\emph{Stieltjes continuity theorem}. Let $(\mu_n)_{n \in Z_+}$ be a 
sequence of random probability measures and $\mu$ a probability measure on the real line. Then following three are equivalent:

(1) $\mu_n$ converges to $\mu$ almost certainly in vague topology.

(2) For any $z \in H_+$ the Stieltjes transform $S_{\mu_n}(z)$ converges to $S_{\mu}(z)$ almost certainly.

(3) Almost surely, for all $z \in H_+$ the Stieltjes transform $S_{\mu_n}(z)$ converges to $S_{\mu}(z)$
\end{theo}

\begin{proof}
(1) $\rightarrow$ (2): Let $z \in H_+$. Define functions $f_N : \R \to \C$ as 

\begin{equation*}
f_N (x) = \frac{1}{x-z} \max(0,1-d(x,[-N,N])).
\end{equation*}

Now we know that almost surely $\int f_N d\mu_n$ converges to $f_N d\mu$. Applying Lebesgue dominated convergence theorem, we obtain that
\begin{equation*}
\int_{\R} \frac{1}{x-z} d\mu_n(x) \to 
\int_{\R} \frac{1}{x-z} d\mu(x)
\end{equation*}
almost surely, so the Stieltjes transform $S_{\mu_n}(z)$ converges to $S_{\mu}(z)$ almost certainly.

(2) $\rightarrow$ (3): It is enough to show that almost surely, the Stieltjes transform $S_{\mu_n}(z)$ converges to $S_{\mu}(z)$ for all $z$ in the rectangles $R_K = [-K,K] \times [1/K,K]$, as the upper half-plane is a countable union of such rectangles.

Fix $K$ and let $(q_i)$ be an enumeration of the rational points of $R_K$. Then for all $i$ $S\mu_n(q_i)$ converges to $S\mu(q_i)$ almost surely.

Consider now the case that every $S\mu_n(q_i)$ converges to $S\mu(q_i)$.
Because there are only countably many points $q_i$, this happens almost surely. Fix $z \in R_K$ and $\eps > 0$. We will show that $S\mu_n(z)$ converges to $S\mu(z)$.

Recall that the derivative of any Stieltjes transform in $R_K$ is bounded by $K^2$, and therefore $S\mu_n$, $S\mu$ are $K^2$-Lipschitz. As the rational points are dense in $R_K$, we can find a $q_i$ with 
$|z-q_i|< \eps /(3K^2)$.

Since we assume that $S\mu_n(q_i)$ converges to $S\mu(q_i)$, we know that for large enough $n$ $|S\mu_n(q_i)-S\mu(q_i)|< \eps / 3$. Combining these estimates gives for large enough $n$
\begin{eqnarray*}
|S\mu_n(z)-S\mu(z)| & = & |S\mu_n(z)-S\mu_n(q_i) + S\mu_n(q_i)-S\mu(q_i)+S\mu(q_i)-S\mu(z)|\\
& \leq & |S\mu_n(z)-S\mu_n(q_i)| + |S\mu_n(q_i)-S\mu(q_i)|+|S\mu(q_i)-S\mu(z)|\\
& \leq & K^2|z-q_i| + \frac{\epsilon}{3} + K^2|q_i-z|\\
& \leq & \frac{K^2\eps}{3K^2} + \frac{\epsilon}{3} + \frac{K^2\eps}{3K^2}\\
& = & \eps
\end{eqnarray*}
and it follows that $S\mu_n(z)$ converges to $S\mu(z)$.

(3) $\rightarrow$ (1): Recall from previous lemma that the imaginary part of the Stieltjes transform can be interpreted as an approximation of identity multiplied by $\pi$.

Let $\phi$ be any continuous compactly supported function, and let $M$ and $K$ be constants such that $|\phi(x)|\leq M$ for any $x$ and $\phi(x)=0$ for $|x|>K$. Fix $\eps > 0$. 
Then for small enough $b>0$, we have $|\phi * P_b (x)-\phi(x)| < \eps$ for all $x$. This means that for any $n$, we have 

\begin{equation*}
|\int_{\R} \phi(x) d\mu_n(x) - \int_{\R} \phi * P_b d\mu_n(x)| \leq \eps
\end{equation*} 

and the same inequality holds if we replace $\mu_n$ with $\mu$.

As in the previous lemma, denote the imaginary parts of functions $a  \to \frac{1}{\pi} S_{\mu}(a+ib)$ and $a  \to \frac{1}{\pi} S_{\mu_n}(a+ib)$ by $f_b$ and $f_{b,n}$, respectively.  We notice that $\int_{\R} \phi * P_b d\mu(x) = \int \phi(x) f_{b}(x) dx$ and $\int_{\R} \phi * P_b d\mu_n(x) = \int \phi(x) f_{b,n}(x) dx$.

By hypothesis, almost surely, for all $x$ $f_{b,n}(x)$ converges to $f_b(x)$. All values $\phi(x) f_{b,n}(x)$ are dominated by $\frac{M}{b} \chi_{[-K,K]} (x)$ so by Lebesgue's dominated convergence theorem, we can almost surely choose $N$ such that

\begin{equation*}
|\int \phi(x) f_{b,n}(x) dx - \int \phi(x) f_{b}(x) dx| \leq \eps 
\end{equation*}
for any $n>N$.

Now combining the two estimates with triangle inequality means that almost surely we can choose $N$ such that
\begin{eqnarray*}
|\int_{\R} \phi(x) d\mu_n(x) - \int_{\R} \phi(x) d\mu(x)| & \leq &
|\int_{\R} \phi(x) d\mu_n(x) - \int_{\R} \phi * P_b d\mu_n(x)|\\
&  & + |\int \phi(x) f_{b,n}(x) dx - \int \phi(x) f_{b}(x) dx|\\
& & + | \int_{\R} \phi * P_b d\mu(x)- \int_{\R} \phi(x) d\mu(x)|\\
& \leq & 3\eps
\end{eqnarray*}

As $\eps$ was arbitrary, we have that 
$\int_{\R} \phi(x) d\mu_n(x)$ converges to $\int_{\R} \phi(x) d\mu(x)$ almost surely.
\end{proof}

\chapter{Deriving the semicircular law}\label{semic}

The objective is clear now: deriving the almost sure convergence of empirical spectral distributions to a limit reduces to proving the almost sure convergence of the Stieltjes tranforms.
If the almost sure limit of Stieltjes transforms exists, we can deduce the limit measure by taking limit with the limit function.

If we denote the $n$ eigenvalues of $W_n$ by $\lambda_1, \ldots, \lambda_n$,  the Stieltjes transform of the ESD can be written as

\begin{eqnarray*}
S\mu_{\frac{1}{\sqrt{n}}W_n}(z) & = & 
\frac{1}{n} \sum_{i=1}^n \frac{1}{\lambda_i/\sqrt{n} - z}\\
& = & \frac{1}{n} tr(\frac{1}{\sqrt{n}}W_n - z I_n)^{-1}.
\end{eqnarray*}

From now on, $z$ will be a fixed complex number with positive imaginary part. We will prove that the Stieltjes transforms converge almost surely in two parts. First we prove that the expectations of the Stieltjes transforms have a limit function, then we prove that the difference of Stieltjes transform and its expectation goes to zero almost surely.

Consider the expectation of the empirical spectral measure.  By linearity of expectation, we have

\begin{eqnarray*}
ES\mu_{\frac{1}{\sqrt{n}}W_n}(z) & = &
E(\frac{1}{n} tr(\frac{1}{\sqrt{n}}W_n - z I_n)^{-1})\\
& = & E(\frac{1}{n} \sum_{l=1}^n(\frac{1}{\sqrt{n}}W_n - z I_n)^{-1}_{ii})\\
& = & \frac{1}{n} \sum_{l=1}^n E(\frac{1}{\sqrt{n}}W_n - z I_n)^{-1}_{ll}.
\end{eqnarray*}

Consider a fixed term in the sum.
Using Schur complement, we have

\begin{equation*}
E(\frac{1}{\sqrt{n}}W_n - z I_n)^{-1}_{ll} = E\frac{1}{-z+\frac{\xi_{l,l}}{\sqrt{n}}-\frac{1}{n}C^* (\frac{1}{\sqrt{n}}W_n^l-zI_{n-1})^{-1}C},
\end{equation*}

where $C$ is the column vector $(\xi_{il})_{i=1, i\neq l}^{n}$, and $W_n^l$ is the matrix $W_n$ with $l$th column and row removed.

First, consider the term $\xi_{ll}/\sqrt{n}$. From zero mean and unit variance hypothesis, it immediately follows by Markov's inequality that

\begin{equation*}
P(|\frac{\xi_{ll}}{\sqrt{n}}|>\frac{\lambda}{\sqrt[4]{n}}) \leq \frac{1}{\sqrt{n}\lambda^2}. 
\end{equation*}

This means informally that the term is usually bounded by $o(1)$. Notice that we could have assumed instead that the diagonal elements have a finite mean and variance, and a similar result would have followed.

Now consider the other random term. Notice that the matrix $A = (\frac{1}{\sqrt{n}}W_n^l-zI_{n-1})^{-1}$ has $n-1$ eigenvalues, and the eigenvalues are bounded below in absolute value by $|Im(z)|^{-1}$. Also, matrix $A$ is independent from the vector $C$. We will consider the concentration of the term $C^* A C$ for a fixed $A$.

\begin{lem}
For a fixed matrix $A$, we have
\begin{equation*}
E(C^* A C) = tr(A).
\end{equation*}
\end{lem}

\begin{proof}
Denote $c_i = \xi_{il}$ if $i<l$ and $c_i = \xi_{(i+1)l}$ if $i\geq l$.
As the random variables $c_i$ are independent and have mean zero and unit variance, we can compute
\begin{eqnarray*}
E(C^* A C) & = & E(\sum_{i=1}^{n-1} \sum_{j=1}^{n-1} \overline{c_i} a_{ij} c_j)\\
& = & \sum_{i=1}^{n-1} \sum_{j=1}^{n-1} E(\overline{c_i} a_{ij} c_j)\\
& = & \sum_{i=1}^{n-1} (a_{ii} E(\overline{c_i}c_i) + \sum_{j=1,j \neq i}^{n-1} a_{ij} E(\overline{c_i} c_j))\\
& = & \sum_{i=1}^{n-1} (a_{ii} E(|c_i|^2 ) + \sum_{j=1,j \neq i}^{n-1} a_{ij} E(\overline{c_i}) E(c_j))\\
& = & \sum_{i=1}^{n-1} (a_{ii} \cdot 1 + \sum_{j=1,j \neq i}^{n-1} a_{ij} \cdot 0)\\
& = & \sum_{i=1}^{n-1} a_{ii}\\
& = & tr(A).
\end{eqnarray*} 
\end{proof}

\begin{lem}
For a fixed hermitian matrix $A$ with operator norm bounded by $c>0$, we have
\begin{equation*}
Var(C^* A C) \leq R \| A \|_2^2 \leq n R c^2.
\end{equation*}
for a constant $R>0$ independent from $A$.
\end{lem}

\begin{proof}
We computed the expectation in the previous lemma. As $Var(Y) = E(|Y|^2)-|E(Y)|^2$, we need to compute $E(|C^* A C|^2)$. Similarly to the previous lemma, we denote the component of $C$ by $c_i$ and compute
\begin{eqnarray*}
E(|C^* A C|^2) & = & E(|\sum_{i=1}^{n-1} \sum_{j=1}^{n-1} \overline{c_i} a_{ij} c_j|^2)\\
& = & E((\sum_{i=1}^{n-1} \sum_{j=1}^{n-1} \overline{c_i} a_{ij} c_j)
(\overline{\sum_{i=1}^{n-1} \sum_{j=1}^{n-1} \overline{c_i} a_{ij} c_j}))\\
& = & E (\sum_{i_1=1}^{n-1}\sum_{i_2=1}^{n-1}
\sum_{j_1=1}^{n-1}\sum_{j_2=1}^{n-1} 
\overline{c_{i_1}} a_{i_1j_1} c_{j_1}c_{i_2} \overline{a_{i_2j_2} c_{j_2}})\\
& = & \sum_{i_1=1}^{n-1}\sum_{i_2=1}^{n-1}
\sum_{j_1=1}^{n-1}\sum_{j_2=1}^{n-1} 
E(\overline{c_{i_1}} a_{i_1j_1} c_{j_1}c_{i_2} \overline{a_{i_2j_2} c_{j_2}}).
\end{eqnarray*}
Now recall that all $c_k$ have mean zero and are independent. This means that if in one term one of the indices $i_1, i_2, j_1$ and $j_2$ is distinct from others, that term vanishes.

The only remaining terms are the ones in one of the following cases:

(1) $i_1 = i_2 = j_1 = j_2$

(2) $i_1=i_2 \neq j_1=j_2$

(3) $i_1 = j_1 \neq i_2 = j_2$

(4) $i_1 = j_2 \neq i_2 = j_1$

Let's go through these cases individually. Denote by $S_i$ the sum of the terms from case $i$.
For case (1), recall that the non-diagonal elements have a finite fourth moment, so $E(|c_{k}|^4) = K < \infty$. Letting $i_1=i_2=j_1=j_2=k$, we have
\begin{eqnarray*}
S_1 & = & \sum_{k=1}^{n-1} E(\overline{c_{k}} a_{kk} c_{k}c_{k} \overline{a_{kk} c_{k}})\\
& = & \sum_{k=1}^{n-1} |a_{kk}|^2 E(|c_{k}|^4)\\
& = & \sum_{k=1}^{n-1} K |a_{kk}|^2
\end{eqnarray*}
In case (2), set $i_1=i_2=k_1$ and $j_1=j_2=k_2$, and we have 
\begin{eqnarray*}
S_2 & = & \sum_{k_1=1}^{n-1} \sum_{k_2=1, k_1\neq k_2}^{n-1} E(\overline{c_{k_1}} a_{k_1k_2} c_{k_2}c_{k_1} \overline{a_{k_1k_2} c_{k_2}})\\
& = & \sum_{k_1=1}^{n-1} \sum_{k_2=1, k_1\neq k_2}^{n-1} |a_{k_1k_2}|^2 E(|c_{k_1}|^2)  E(|c_{k_2}|^2)\\
& = & \sum_{k_1=1}^{n-1} \sum_{k_2=1, k_1\neq k_2}^{n-1} |a_{k_1k_2}|^2
\end{eqnarray*}
In case (3), setting $i_1=j_1=k_1$ and $i_2=j_2=k_2$ gives
\begin{eqnarray*}
S_3 & = & \sum_{k_1=1}^{n-1} \sum_{k_2=1, k_1\neq k_2}^{n-1} E(\overline{c_{k_1}} a_{k_1k_1} c_{k_1}c_{k_2} \overline{a_{k_2k_2} c_{k_2}})\\
& = & \sum_{k_1=1}^{n-1} \sum_{k_2=1, k_1\neq k_2}^{n-1} a_{k_1k_1} \overline{a_{k_2k_2}} E(|c_{k_1}|^2)  E(|c_{k_2}|^2)\\
& = & \sum_{k_1=1}^{n-1} \sum_{k_2=1, k_1\neq k_2}^{n-1} a_{k_1k_1} \overline{a_{k_2k_2}}
\end{eqnarray*}
Notice that the sums in previous cases have been real. For (1) and (2) all the terms are real, and case (3) may be written as
\begin{equation*}
S_3 = \sum_{k_1=1}^{n-1} \sum_{k_2=k_1+1}^{n-1} (a_{k_1k_1} \overline{a_{k_2k_2}}+ \overline{a_{k_1k_1}} a_{k_2k_2}).
\end{equation*}
Finally in case (4), setting $i_1=j_2=k_1$ and $i_2=j_1=k_2$ to compute
\begin{eqnarray*}
S_4 & = & \sum_{k_1=1}^{n-1}\sum_{k_2=1, k_1\neq k_2}^{n-1}
E(\overline{c_{k_1}} a_{k_1k_2} c_{k_2}c_{k_2} \overline{a_{k_2k_1} c_{k_1}})\\
& = & \sum_{k_1=1}^{n-1}\sum_{k_2=1, k_1 \neq k_2}^{n-1} a_{k_1k_2} \overline{a_{k_2k_1}}
E(\overline{c_{k_1}}^2) E( c_{k_2}^2)
\end{eqnarray*}
We know that $S_4$ must be real. Using triangle inequality and arithmetic-geometric mean inequality, we estimate
\begin{eqnarray*}
S_4 & = & \sum_{k_1=1}^{n-1}\sum_{k_2=1, k_1 \neq k_2}^{n-1} a_{k_1k_2} \overline{a_{k_2k_1}}
E(\overline{c_{k_1}}^2) E( c_{k_2}^2)\\
& \leq & \sum_{k_1=1}^{n-1}\sum_{k_2=1, k_1 \neq k_2}^{n-1} |a_{k_1k_2}| |a_{k_2k_1}|
|E(\overline{c_{k_1}}^2)| |E( c_{k_2}^2)|\\
& \leq & \sum_{k_1=1}^{n-1}\sum_{k_2=1, k_1 \neq k_2}^{n-1} |a_{k_1k_2}| |a_{k_2k_1}|
E(|c_{k_1}|^2) E(|c_{k_2}|^2)\\
& = & \sum_{k_1=1}^{n-1}\sum_{k_2=1, k_1 \neq k_2}^{n-1} |a_{k_1k_2}| |a_{k_2k_1}|\\
& \leq & \sum_{k_1=1}^{n-1}\sum_{k_2=1, k_1 \ neq k_2}^{n-1} \frac{|a_{k_1k_2}|^2+|a_{k_2k_1}|^2}{2}\\
& = & \sum_{k_1=1}^{n-1}\sum_{k_2=1, k_1 \neq k_2}^{n-1} |a_{k_1k_2}|^2.
\end{eqnarray*}

Now
\begin{eqnarray*}
Var(C^* A C) & = & S_1+S_2+S_3+S_4 - |E(C^* A C)|^2\\
& = & \sum_{k=1}^{n-1} (K-1) |a_{kk}|^2 + |tr(A)|^2 + S_2+S_4 - |tr(A)|^2\\
& = &  \sum_{k=1}^{n-1} (K-1) |a_{kk}|^2 + \sum_{k_1=1}^{n-1} \sum_{k_2=1, k_1\neq k_2}^{n-1} |a_{k_1k_2}|^2+ S_4\\
& \leq & \sum_{k=1}^{n-1} (K-1) |a_{kk}|^2 + \sum_{k_1=1}^{n-1} \sum_{k_2=1, k_1\neq k_2}^{n-1} 2|a_{k_1k_2}|^2\\
& \leq & \max(K-1,2) \sum_{k_1=1}^{n-1} \sum_{k_2=1}^{n-1} |a_{k_1k_2}|^2\\
& = & R \|A\|_2^2. 
\end{eqnarray*}
Observe that the constant $R = \max(K-1,2)$ does not depend on $n$ or $A$. If the operator norm of $A$ is bounded by $c$, then $|Ae_i|^2\leq c^2$. But this means that
\begin{eqnarray*}
R \|A\|_2^2 & = & R \sum_{k_1=1}^{n-1} \sum_{k_2=1}^{n-1} |a_{k_1k_2}|^2\\
& = & R \sum_{k_2=1}^{n-1} \sum_{k_1=1}^{n-1} |a_{k_1k_2}|^2\\
& = & R \sum_{k_2=1}^{n-1} |Ae_{k_2}|^2\\
& \leq & R n c^2
\end{eqnarray*}
\end{proof}

We can now apply Chebyshev's inequality to conclude $P(|C^* A C - tr(A)|>\lambda) \leq nRc^2/\lambda^2$, or equivalently $P(|C^* A C - tr(A)|>n^{\frac{3}{4}}\lambda) \leq Rc^2/\sqrt{n}\lambda^2$. We now have

\begin{equation*}
ES\mu_{\frac{1}{\sqrt{n}}W_n}(z) = -E\frac{1-o(1)}{z+\frac{1}{n}tr(\frac{1}{\sqrt{n}}W_n^l-zI_{n-1})^{-1} + o(1)} + o(1)Im(z)^{-1},
\end{equation*}

where the magnitude of the error terms does not depend on $l$. Next, we will approximate the term $\frac{1}{n}tr(\frac{1}{\sqrt{n}}W_n^l-zI_{n-1})^{-1}$ by the original Stieltjes transform $S\mu_{\frac{1}{\sqrt{n}}W_n}(z)$.

\begin{lem}
For any $l$ we have the estimate
\begin{equation*}
|\frac{1}{n}tr(\frac{1}{\sqrt{n}}W_n^l-zI_{n-1})^{-1} - S_{\mu_{\frac{1}{\sqrt{n}}W_n}}(z)| \leq \frac{C_z}{n}.
\end{equation*}
\end{lem}

\begin{proof}
Knowing that both $W_n$ and $W_n^l$ are hermitian matrices, let $\lambda_1 \leq \lambda_2 \leq \ldots \lambda_n$ be the eigenvalues of $W_n$ and $\lambda_1' \leq \lambda_2' \leq \ldots \lambda_{n-1}'$ be the eigenvalues of $W_n^l$. We know by Cauchy interlacing formula that

\begin{equation*}
\lambda_{i} \leq \lambda_{i}' \leq \lambda_{i+1} (A)
\end{equation*}

for $1\leq i \leq n-1$.

Now we see that the eigenvalues of $(\frac{1}{\sqrt{n}}W_n^l-zI_{n-1})^{-1}$ are $(\lambda_i'/\sqrt{n}-z)^{-1}$ for $1\leq i \leq n-1$. Rewriting the difference we want to estimate and using triangle inequality gives

\begin{eqnarray*}
\left|\frac{1}{n}tr(\frac{1}{\sqrt{n}}W_n^l-zI_{n-1})^{-1} - S_{\mu_{\frac{1}{\sqrt{n}}W_n}}(z)\right| & = & \frac{1}{n}
\left|\sum_{i=1}^{n-1} \frac{1}{\frac{\lambda_i'}{\sqrt{n}}-z} - \sum_{i=1}^{n} \frac{1}{\frac{\lambda_i}{\sqrt{n}}-z}\right|\\
& \leq & \frac{1}{n}
\sum_{i=1}^{n-1} \left|\frac{1}{\frac{\lambda_i'}{\sqrt{n}}-z} - \frac{1}{\frac{\lambda_i}{\sqrt{n}}-z}\right| + \left|\frac{1}{\frac{\lambda_n}{\sqrt{n}}-z}\right|.
\end{eqnarray*}

Now we note that the intervals $]\lambda_i/\sqrt{n},\lambda_i'/\sqrt{n}[$ are disjoint by Cauchy interlacing formula. For any sequence $a_1 < a_2 < ... < a_{2m-1} < a_{2m}$, we have 

in other words, the function $x \to (x-z)^1$ has bounded variation. 
\end{proof}

\begin{lem}
For any index $n$ and positive number $\lambda$, we have
\begin{equation*}
P(|S\mu_{\frac{1}{\sqrt{n}}W_n}(z)- ES\mu_{\frac{1}{\sqrt{n}}W_n}(z)|>\lambda) \leq Ce^{-cn\lambda^2}.
\end{equation*}
for some constants $C, c > 0$, depending only on $z$.
\end{lem}

\begin{proof}
zzzz
\end{proof}

By continuity (as the denominator is bounded away from zero) 

\begin{equation*}
ES\mu_{\frac{1}{\sqrt{n}}W_n}(z) = -\frac{1}{z+ES\mu_{\frac{1}{n}W_n}(z)} + o(1).
\end{equation*}

Solving this, we get
\begin{equation*}
ES\mu_{\frac{1}{\sqrt{n}}W_n}(z) = \frac{-z \pm \sqrt{z^2 - 4}}{2} +o(1).
\end{equation*}

Since $S\mu$ maps the upper half-plane to the upper half-plane,  

\begin{equation*}
ES\mu_{\frac{1}{\sqrt{n}}W_n}(z) = \frac{-z + \sqrt{z^2 - 4}}{2} +o(1),
\end{equation*}

where the branch of the square root has positive imaginary part in upper half-plane.

With this it is already possible to prove a slightly weaker version of the semicircular law concerning the converge of the expectation measures $E\mu_n$, defined as $(E\mu)(A) = E(\mu(A))$. However, we can also recover almost sure convergence with following lemma, which is a special case of \emph{McDiarmid's inequality}.

Recall.

Now, the sum $\sum_{n=1}^{\infty} Ce^{-cn\varepsilon}$ converges as a geometric series. Combining previous lemma with Borel-Cantelli lemma implies that almost surely

\begin{equation*}
S\mu_{\frac{1}{\sqrt{n}}W_n}(z) \to g(z) = \frac{-z + \sqrt{z^2-4}}{2}.
\end{equation*}

Now investigating the behaviour of imaginary part near real axis has

\begin{eqnarray*}
\frac{1}{\pi} Im(g(x+yi)) & = &
\frac{y}{2\pi} + \frac{\sqrt{\frac{4-x^2+y^2+\sqrt{(x^2-y^2-4)^2+4x^2y^2}}{2}}}{2\pi}\\
& \to & \frac{1}{2\pi} \sqrt{(4-x^2)_+},
\end{eqnarray*}

meaning that the limit of the imaginary part of the function is the \emph{Wigner semicircular distribution}. Now that we know what the limiting probability measure should be, the problem of verifying the limit measure reduces to straightforward computation.

\begin{theo}
Let $\mu_{sc}$ be the probability measure of the Wigner semicircular distribution. Then
\begin{equation*}
S\mu_{sc}(z) = g(z).
\end{equation*}
\end{theo}

\begin{proof}
We will compute the Stieltjes transform at point $z$ in upper half-plane. First, making the substitution $x = 2 \cos \theta$, we get
\begin{eqnarray*}
S\mu_{sc}(z) & = & \int_{\R} \frac{1}{x-z} d\mu_{sc}(x)\\
& = & \frac{1}{2\pi} \int_{-2}^2 \frac{\sqrt{4-x^2}}{x-z} dx\\
& = & \frac{1}{2\pi} \int_{0}^{\pi} \frac{\sqrt{4-4\cos^2\theta}}{2\cos\theta -z} 2 \sin\theta d\theta\\
& = & \frac{1}{2\pi} \int_{0}^{\pi} \frac{4\sin^2\theta}{2\cos\theta -z} d\theta\\
& = & \frac{1}{4\pi} \int_{0}^{2\pi} \frac{4\sin^2\theta}{2\cos\theta -z} d\theta.
\end{eqnarray*}

Expressing the sine and cosine via complex exponential function and making substitution $e^{i\theta} = w$, we get
\begin{eqnarray*}
S\mu_{sc}(z) & = & \frac{1}{4\pi} \int_{0}^{2\pi} \frac{4\sin^2\theta}{2\cos\theta -z} d\theta\\
& = & \frac{1}{4\pi} \int_{0}^{2\pi} \frac{4(\frac{1}{2i}(e^{i\theta}-e^{-i\theta}))^2}{2(\frac{1}{2}(e^{i\theta}+e^{-i\theta})) -z} d\theta\\
& = & \frac{1}{4\pi} \int_{0}^{2\pi} \frac{-(e^{2i\theta}-2+e^{-2i\theta})}{(e^{i\theta}+e^{-i\theta}) -z} d\theta\\
& = & \frac{1}{4\pi} \int_{|w|=1} \frac{2-w^2-w^{-2}}{(w+w^{-1}) -z} \frac{dw}{iw}\\
& = & \frac{1}{4\pi} \int_{|w|=1} \frac{2-w^2-w^{-2}}{(w+w^{-1}) -z} \frac{dw}{iw}\\
& = & \frac{1}{4\pi i} \int_{|w|=1} \frac{2-w^2-w^{-2}}{w^2+1 -zw} dw\\
& = & \frac{1}{4\pi i} \int_{|w|=1} \frac{-w^4+2w^2-1}{w^2(w^2+1 -zw)} dw\\
& = & \frac{1}{4\pi i} \int_{|w|=1} h_z(w) dw.
\end{eqnarray*}

We are going to apply the residue theorem to compute the integral. As the function $h_z$ has poles at $w=0$ and $w=\frac{z \pm \sqrt{z^2-4}}{2}$, we need to figure out if the poles at $w=\frac{z \pm \sqrt{z^2-4}}{2}$ lie in the unit disk.

Denote those two poles by $p_1$ and $p_2$. Then, $p_1 + p_2 = z$ and $p_1p_2=1$, which implies that exactly one of them lies in the unit disk. Since $z$ has positive imaginary part, the one with smaller absolute value must have negative imaginary part.
Therefore, the pole $w=\frac{z - \sqrt{z^2-4}}{2} = p_1$ lies in the unit disk, where the square root branch has positive imaginary part.

Calculating the residues is straightforward. For $w=0$:

\begin{eqnarray*}
Res(h_z,0) & = &
\left(\frac{d}{dw} \frac{-w^4+2w^2-1}{w^2-zw+1} \right)_{w=0}\\
& = & \left(\frac{(-4w^3+4w)(w^2-zw+1)-(-w^4+2w^2-1)(2w-z)}{(w^2-zw+1)^2} \right)_{w=0}\\
& = & \frac{(-4\cdot 0^3+4 \cdot 0)(0^2-z\cdot 0+1)-(-0^4+2\cdot 0^2-1)(2\cdot 0-z)}{(0^2-z\cdot 0+1)^2} \\
& = & -z
\end{eqnarray*}

For $w=p_1$, we use the fact that $p_1p_2 = 1$:

\begin{eqnarray*}
Res(h_z,p_1) & = &
\left(\frac{-w^4+2w^2-1}{w^2(w-p_2)} \right)_{w=p_1}\\
& = & \frac{-p_1^4+2p_1^2-1}{p_1^2(p_1-p_2)}\\
& = & \frac{-(p_1^2-1)^2}{p_1^2(p_1-p_2)}\\
& = & \frac{-(p_1^2-p_1p_2)^2}{p_1^2(p_1-p_2)}\\
& = & \frac{-p_1^2(p_1-p_2)^2}{p_1^2(p_1-p_2)}\\
& = & p_2-p_1\\
& = & \frac{z + \sqrt{z^2-4}}{2} - \frac{z - \sqrt{z^2-4}}{2}\\
& = & \sqrt{z^2-4}.
\end{eqnarray*}

Now by residue theorem we have

\begin{eqnarray*}
S\mu_{sc}(z) & = & \frac{1}{4\pi i} \int_{|w|=1} \frac{-w^4+2w^2-1}{w^2(w^2+1 -zw)} dw\\
& = & \frac{1}{2} ( -z + \sqrt{z^2-4}).
\end{eqnarray*}

\end{proof}

By Stieltjes continuity theorem, we deduce that $\mu_{\frac{1}{\sqrt{n}}W_n}$ converges to the semicircular distribution almost surely in vague topology. We have finally proved:

\begin{theo}
\emph{Wigner semicircular law.} For an ensemble of Wigner matrices $(W_n)_{n=1}^{\infty}$ with all elements of matrices being independent and identically distributed, having zero mean, unit variance and bounded fourth moment, the empirical spectral distributions $\mu_{\frac{1}{\sqrt{n}}W_n}$ converges almost surely to Wigner semicircular distribution $\mu_{sc}$.
\end{theo}


\chapter{What next?}\label{future}

The future is here.

As was mentioned before, it is also possible to prove the semicircular law by considering the moments of the empirical spectral distribution. Let us prove a slightly modified version of the semicircular law by this method.

\begin{theo}
Let $(\xi_{ij})_{i,j \in \Z_+}$ be random variables (not necessarily identically distributed) such that $\xi_{ij}$
are independent with mean zero and unit variance for $i \leq j$, with diagonal $\xi_{ii}$ real random variables, and $\xi_{ij}=\overline{\xi_{ji}}$ for $i>j$. Assume also that $|\xi_{ij}|<M$ almost surely for some absolute constant $M$, in other words the random variables are uniformly bounded almost certainly.
Then, defining Wigner matrix $W_n$ as before, the empirical spectral distributions $\mu_{\frac{1}{\sqrt{n}}W_n}$ converge to $\mu_{sc}$ almost certainly.
\end{theo}

The following result reduces this claim in a way similar to Stieltjes continuity theorem:

\begin{thebibliography}{9}

\bibitem{Tao}
Terence Tao: Topics in random matrix theory, 

\bibitem{Duo}
Javier Duoandikoetxea: Fourier analysis, 2.\ painos, Limes ry, 1990.

\end{thebibliography}

\end{document}
