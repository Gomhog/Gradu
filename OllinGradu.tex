\documentclass[12pt,a4paper,leqno]{report}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsthm}
\usepackage{amsfonts}         
\usepackage{amsmath}
\usepackage{amssymb}

\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\No}{\mathbb{N}_0}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\diam}{\operatorname{diam}}

\newcommand{\eps}{\varepsilon}
\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}

\theoremstyle{plain}
\newtheorem{theo}[equation]{Theorem}
\newtheorem{lem}[equation]{Lemma}
\newtheorem{prop}[equation]{Proposition}
\newtheorem{cor}[equation]{Corollary}

\theoremstyle{definition}
\newtheorem{defi}[equation]{Definition}
\newtheorem{conjec}[equation]{Conjecture}
\newtheorem{exa}[equation]{Example}

\theoremstyle{remark}
\newtheorem{rema}[equation]{Remark}

\pagestyle{plain}
\setcounter{page}{1}
\addtolength{\hoffset}{-1.15cm}
\addtolength{\textwidth}{2.3cm}
\addtolength{\voffset}{0.45cm}
\addtolength{\textheight}{-0.9cm}

\title{Random matrix stuff}
\author{Olli Hirviniemi}
\date{}

\begin{document}

\maketitle

\tableofcontents

\chapter{Introduction}
\label{intro}

A random matrix is a matrix-valued random variable. The idea of a random matrix dates back to the first half of the 20th century. Wishart \cite{Wis} used random matrices in statistics and von Neumann \cite{Neu} studied random matrices to estimate numerical errors.

It was Eugene Wigner \cite{Wig1} who noticed the connection between eigenvalues of symmetric random matrices and spectrum of a heavy atom nucleus. He laid some of the first mathematical foundations of the theory \cite{Wig2} and proved a result that would be known as Wigner's semicircular law. This was a major breakthrough and motivation for the study of random matrices.

Later applications of random matrix theory include telephone encryption and many topics in theoretical physics such as quantum chaos and quantum gravity. \cite{Diac}. The eigenvalues of random matrices also appear to be connected to the zeroes of the Riemann zeta function. \cite{Rud} The random matrices have many nice and interesting properties that not only answer the questions posed by applications but are also beautiful on their own.

We will begin by deriving the semicircular law for random hermitian matrices by using Stieltjes transform. The outline of the proof is as in \cite{Tao} with some small modifications.



\chapter{Wigner matrices}
\label{setup}

\begin{defi}
Suppose that $(\xi_{ij})_{i,j \in \Z_+}$ are random variables with $(\xi_{ij})_{i \leq j}$ being independent, $(\xi_{ii})_{i \in \Z_+}$ identically distributed real random variables, $(\xi_{ij})_{i<j}$ identically distributed complex random variables, and $\xi_{ij}=\overline{\xi_{ji}}$ for $i>j$. With such random variables, an associated \emph{Wigner matrix} (of size $n$) is the hermitian matrix
\begin{equation*}
W_n = (\xi_{ij})_{1\leq i,j \leq n}.
\end{equation*} 
\end{defi}

Assume that every $\xi_{ij}$ has mean $0$ and unit variance. We will also assume that the fourth moment of the off-diagonal elements is bounded by some constant $K$: this is true in many important special cases such as Gaussian matrix ensembles.

Since $W_n$ is hermitian, it has $n$ real eigenvalues by spectral theorem.
Trying to understand how these eigenvalues are distributed is the purpose of this section: namely, deriving the \emph{semicircular law}.

First consider the magnitude of the eigenvalues. A simple example of $n \times n$ -matrix of ones shows that even with bounded coefficients, the eigenvalues can grow in magnitude as the size of the matrix increases. So there is need to normalize the matrix somehow.

Following lemma motivates the normalizing factor used:

\begin{lem}
Let $x$ be a unit vector of $\C^n$. Then for any $\lambda > 0$, 
\begin{equation*}
P(|W_nx|>\sqrt{n}\lambda) \leq \frac{8}{\lambda^2}.
\end{equation*}
\end{lem}

\begin{proof}
Fix a unit vector $x$. Split $W_n = M_1 + M_2$, where $M_1$ consist of upper triangular elements of $W_n$ (so $M_1$ is zero below the main diagonal).
If we denote the components of $x$ with $x_i$ and rows of $M_1$ by $R_i$, then we get following upper bound using independence and zero mean hypothesis:
\begin{eqnarray*}
E(|M_1x|^2) & = &  
E(\sum_{i=1}^n |R_i \cdot x|^2) \\
& = & \sum_{i=1}^n E(|R_i \cdot x|^2)\\
& = & \sum_{i=1}^n 
E(|\sum_{j=i}^n \xi_{ij} x_j|^2)\\
& = & \sum_{i=1}^n 
E(\sum_{j=i}^n \sum_{k=i}^n \xi_{ij} x_j\overline{\xi_{ik} x_k}) \\
& = & \sum_{i=1}^n 
\sum_{j=i}^n \sum_{k=i}^n E(\xi_{ij} x_j\overline{\xi_{ik} x_k}) \\
& = & \sum_{i=1}^n 
\sum_{j=i}^n |x_j|^2E(|\xi_{ij}|^2)\\
& = & \sum_{i=1}^n \sum_{j=i}^n |x_j|^2\\
& \leq & \sum_{i=1}^n \sum_{j=1}^n |x_j|^2\\
& = & \sum_{i=1}^n 1\\
& = & n
\end{eqnarray*}
Now by Markov's inequality 
\begin{equation*}
P(|M_1x|^2 \geq \lambda^2) \leq  \frac{n}{\lambda^2},
\end{equation*}
or equivalently
\begin{equation*}
P(|M_1x| \geq \sqrt{n} \lambda) \leq  \frac{1}{\lambda^2}.
\end{equation*}
Same argument also applies for $M_2$ with summing $j$ and $k$ from $1$ to $i-1$, and we have
\begin{equation*}
P(|M_2x| \geq \sqrt{n} \lambda) \leq  \frac{1}{\lambda^2}. 
\end{equation*}
Using triangle inequality, we can estimate
\begin{eqnarray*}
P(|W_nx| \geq \sqrt{n} \lambda) & \leq & P(|M_1x|+|M_2x| \geq \sqrt{n} \lambda)\\
& \leq & P(|M_1x| \geq \sqrt{n} \frac{\lambda}{2}) + P(|M_2x| \geq \sqrt{n} \frac{\lambda}{2})\\
& \leq & \frac{4}{\lambda^2} + \frac{4}{\lambda^2}\\
& = & \frac{8}{\lambda^2}.
\end{eqnarray*}
\end{proof} 


Motivated by the previous lemma, consider now matrices $\frac{1}{\sqrt{n}}W_n$. Looking at the distribution of the eigenvalues, some kind of limit behaviour seems to apply. To handle this rigorously a precise definition for convergence is necessary.

\begin{defi}
Given a hermitian matrix $A$ with eigenvalues $\lambda_1, \ldots , \lambda_n$, the empirical spectral distribution of $A$ is a probability measure given by
\begin{equation*}
\mu_A := \frac{1}{n} \sum_{i=1}^n \delta_{\lambda_i},
\end{equation*}
where $\delta_x$ is a Dirac measure for $x \in \R$.
\end{defi}

Recall that a sequence of measures $\mu_n$ is said to converge to $\mu$ in vague topology if for every compactly supported continuous function $\phi$, $\int_{\R} \phi d\mu_n$ converges to $\int_{\R}\phi d\mu$. When $A$ is a random hermitian matrix, $\mu_A$ is a random probability measure.

\begin{defi}
Let $(\mu_n)_{n=1}^{\infty}$ be a sequence of random measures.
If for any test function $\phi$, the sequence $\int_{\R} \phi d\mu_n$ converges to $\int_{\R} \phi d\mu$ almost certainly, the we say that $\mu_n$ converges to $\mu$ almost certainly.
\end{defi}

\section{Stieltjes transform}
\label{stieltjes}


It is possible to obtain the semicircular law in different ways, for example by considering the moments of $\mu_{\frac{1}{\sqrt{n}}W_n}$. We will return to the moments later. Now define the following tool, the Stieltjes transform:

\begin{defi}
For a probability measure $\mu$ on real line, define its \emph{Stieltjes transform} $S_{\mu}$ to be a complex function defined in upper half plane $H_+$ (and thus outside the support of $\mu$) as
\begin{equation*}
S_{\mu} (z) := \int_{\R} \frac{1}{x-z} d \mu(x).
\end{equation*}
\end{defi}

We will first list some basic properties of the Stieltjes transform.

\begin{lem}
For any probability measure $\mu$, the Stieltjes transform has following properties at any point $z \in H_+$:

\begin{equation}\label{prop1}
\Im(S\mu(z)) > 0
\end{equation}

\begin{equation}\label{prop2}
|S\mu(z)|\leq \frac{1}{\Im(z)} 
\end{equation}

$S\mu$ is analytic in $H_+$ and

\begin{equation}\label{prop3}
|S\mu'(z)| \leq \frac{1}{(\Im(z))^2}
\end{equation}
\end{lem}

\begin{proof}
To prove \ref{prop1}, we observe that for any $z = a + ib \in H_+$ and $x \in \R$ we have
\begin{equation*}
\Im\frac{1}{x-z} = \Im\frac{(x-a) +ib}{(x-a)^2+b^2} = \frac{b}{(x-a)^2+b^2} > 0.
\end{equation*}
Since $\mu$ is a probability measure, we immediately obtain \ref{prop1}.

Next, we can estimate

\begin{eqnarray*}
|S\mu(z)| & = & \left| \int_{\R} \frac{1}{x-z} d \mu(x)  \right|\\
& \leq & \int_{\R} \frac{1}{|x-z|} d \mu(x)\\
& = & \int_{\R} \frac{1}{|x-a-ib|} d \mu(x)\\
& = & \int_{\R} \frac{1}{\sqrt{(x-a)^2+b^2}} d \mu(x)\\
& \leq & \int_{\R} \frac{1}{b} d \mu(x)\\
& = & \frac{1}{b}\\
& = & \Im(z),
\end{eqnarray*}

obtaining \ref{prop2}.

For any fixed $x \in \R$, the function $z \to (x-z)^1$ is analytic in upper half-plane. As the upper half-plane is simply connected, this means that for any closed piecewise smooth curve $\gamma$ in $H_+$ we have

\begin{equation*}
\int_{\gamma} \frac{1}{x-z}dz = 0.
\end{equation*}

Using Fubini's theorem, we get
\begin{eqnarray*}
\int_\gamma S\mu(z) dz & = & 
\int_{\gamma} \int_{\R} \frac{1}{x-z} d\mu(x) dz\\
& = & \int_{\R} \int_{\gamma} \frac{1}{x-z} dz d\mu(x)\\
& = & \int_{\R} 0 d\mu(x)\\
& = & 0,
\end{eqnarray*}
so $S\mu$ is analytic by Moreira's theorem.

Using Fubini's theorem and Cauchy's integral formula, we have for sufficiently small $r$ 

\begin{eqnarray*}
S\mu '(z) & = & \frac{1}{2\pi i}
\int_{\partial B(z,r)} \frac{S\mu(w)}{(w - z)^2} dw\\
& = & \frac{1}{2\pi i}
\int_{\partial B(z,r)} \int_{\R} \frac{\frac{1}{x-z}}{(w - z)^2} d\mu(x) dw\\
& = &
\int_{\R} \frac{1}{2\pi i} \int_{\partial B(z,r)} \frac{\frac{1}{x-z}}{(w - z)^2} dw d\mu(x)\\
& = & \int_{\R} \left( \frac{d}{dz} \frac{1}{x-z} \right) d\mu(x)\\
& = & \int_{\R}\frac{1}{(x-z)^2} d\mu(x).
\end{eqnarray*}

Using the estimate $|x-z| \geq \Im(z)$ gives \ref{prop3}.

\end{proof}

The following two theorems demonstrate why the Stieltjes transform is helpful when investigating the limit behaviour of probability measures. The first allows recovering the probability measure from its Stieltjes transform and the second links the convergence of the measures to the convergence of their Stieltjes transforms.

\begin{theo}
Let $\mu$ be a probability measure on the real line, and $f_b : \R -> \R$ be the imaginary part of the function $a  \to \frac{1}{\pi} S_{\mu}(a+ib)$.  For $\phi$, we have
\begin{equation*}
\lim_{b \to 0} \int_{\R} \phi(x) f_b (x) dx = \int_{\R} \phi(x) d\mu(x).
\end{equation*}
\end{theo}

\begin{proof}
The imaginary part of the integral kernel in Stieltjes transform for $z=a+ib$ is
\begin{equation*}
\frac{b}{(x-a)^2+b^2}.
\end{equation*}
This means that

\begin{eqnarray*}
\lim_{b \to 0} \int_{\R} \phi(x) f_b (x) dx & = & \lim_{b \to 0} \int_{\R} \phi(x) \int_{\R} \frac{b}{(y-x)^2+b^2} d\mu(y) dx\\
& = & \lim_{b \to 0} \int_{\R} \int_{\R} \frac{b}{(y-x)^2+b^2} \phi(x) dx d\mu(y)\\
& = & \int_{\R} \phi(y) d\mu(y).
\end{eqnarray*}
The last equality holds because the kernels $\frac{b}{\pi((x-a)^2+b^2)}$ are an approximation of identity.
\end{proof}

\begin{theo}
\emph{Stieltjes continuity theorem}. Let $(\mu_n)_{n \in Z_+}$ be a 
sequence of random probability measures and $\mu$ a probability measure on the real line. Then following three are equivalent:

(1) $\mu_n$ converges to $\mu$ almost certainly in vague topology.

(2) For any $z \in H_+$ the Stieltjes transform $S_{\mu_n}(z)$ converges to $S_{\mu}(z)$ almost certainly.

(3) Almost surely, for all $z \in H_+$ the Stieltjes transform $S_{\mu_n}(z)$ converges to $S_{\mu}(z)$
\end{theo}

\begin{proof}
(1) $\rightarrow$ (2): Let $z \in H_+$. Define functions $f_N : \R \to \C$ as 

\begin{equation*}
f_N (x) = \frac{1}{x-z} \max(0,1-d(x,[-N,N])).
\end{equation*}

Now we know that almost surely $\int f_N d\mu_n$ converges to $f_N d\mu$. Applying Lebesgue dominated convergence theorem, we obtain that
\begin{equation*}
\int_{\R} \frac{1}{x-z} d\mu_n(x) \to 
\int_{\R} \frac{1}{x-z} d\mu(x)
\end{equation*}
almost surely, so the Stieltjes transform $S_{\mu_n}(z)$ converges to $S_{\mu}(z)$ almost certainly.

(2) $\rightarrow$ (3): It is enough to show that almost surely, the Stieltjes transform $S_{\mu_n}(z)$ converges to $S_{\mu}(z)$ for all $z$ in the rectangles $R_K = [-K,K] \times [1/K,K]$, as the upper half-plane is a countable union of such rectangles.

Fix $K$ and let $(q_i)$ be an enumeration of the rational points of $R_K$. Then for all $i$ $S\mu_n(q_i)$ converges to $S\mu(q_i)$ almost surely.

Consider now the case that every $S\mu_n(q_i)$ converges to $S\mu(q_i)$.
Because there are only countably many points $q_i$, this happens almost surely. Fix $z \in R_K$ and $\eps > 0$. We will show that $S\mu_n(z)$ converges to $S\mu(z)$.

Recall that the derivative of any Stieltjes transform in $R_K$ is bounded by $K^2$, and therefore $S\mu_n$, $S\mu$ are $K^2$-Lipschitz. As the rational points are dense in $R_K$, we can find a $q_i$ with 
$|z-q_i|< \eps /(3K^2)$.

Since we assume that $S\mu_n(q_i)$ converges to $S\mu(q_i)$, we know that for large enough $n$ $|S\mu_n(q_i)-S\mu(q_i)|< \eps / 3$. Combining these estimates gives for large enough $n$
\begin{eqnarray*}
|S\mu_n(z)-S\mu(z)| & = & |S\mu_n(z)-S\mu_n(q_i) + S\mu_n(q_i)-S\mu(q_i)+S\mu(q_i)-S\mu(z)|\\
& \leq & |S\mu_n(z)-S\mu_n(q_i)| + |S\mu_n(q_i)-S\mu(q_i)|+|S\mu(q_i)-S\mu(z)|\\
& \leq & K^2|z-q_i| + \frac{\epsilon}{3} + K^2|q_i-z|\\
& \leq & \frac{K^2\eps}{3K^2} + \frac{\epsilon}{3} + \frac{K^2\eps}{3K^2}\\
& = & \eps
\end{eqnarray*}
and it follows that $S\mu_n(z)$ converges to $S\mu(z)$.

(3) $\rightarrow$ (1): Recall from previous lemma that the imaginary part of the Stieltjes transform can be interpreted as an approximation of identity multiplied by $\pi$.

Let $\phi$ be any continuous compactly supported function, and let $M$ and $K$ be constants such that $|\phi(x)|\leq M$ for any $x$ and $\phi(x)=0$ for $|x|>K$. Fix $\eps > 0$. 
Then for small enough $b>0$, we have $|\phi * P_b (x)-\phi(x)| < \eps$ for all $x$. This means that for any $n$, we have 

\begin{equation*}
|\int_{\R} \phi(x) d\mu_n(x) - \int_{\R} \phi * P_b d\mu_n(x)| \leq \eps
\end{equation*} 

and the same inequality holds if we replace $\mu_n$ with $\mu$.

As in the previous lemma, denote the imaginary parts of functions $a  \to \frac{1}{\pi} S_{\mu}(a+ib)$ and $a  \to \frac{1}{\pi} S_{\mu_n}(a+ib)$ by $f_b$ and $f_{b,n}$, respectively.  We notice that $\int_{\R} \phi * P_b d\mu(x) = \int \phi(x) f_{b}(x) dx$ and $\int_{\R} \phi * P_b d\mu_n(x) = \int \phi(x) f_{b,n}(x) dx$.

By hypothesis, almost surely, for all $x$ $f_{b,n}(x)$ converges to $f_b(x)$. All values $\phi(x) f_{b,n}(x)$ are dominated by $\frac{M}{b} \chi_{[-K,K]} (x)$ so by Lebesgue's dominated convergence theorem, we can almost surely choose $N$ such that

\begin{equation*}
|\int \phi(x) f_{b,n}(x) dx - \int \phi(x) f_{b}(x) dx| \leq \eps 
\end{equation*}
for any $n>N$.

Now combining the two estimates with triangle inequality means that almost surely we can choose $N$ such that
\begin{eqnarray*}
|\int_{\R} \phi(x) d\mu_n(x) - \int_{\R} \phi(x) d\mu(x)| & \leq &
|\int_{\R} \phi(x) d\mu_n(x) - \int_{\R} \phi * P_b d\mu_n(x)|\\
&  & + |\int \phi(x) f_{b,n}(x) dx - \int \phi(x) f_{b}(x) dx|\\
& & + | \int_{\R} \phi * P_b d\mu(x)- \int_{\R} \phi(x) d\mu(x)|\\
& \leq & 3\eps
\end{eqnarray*}

As $\eps$ was arbitrary, we have that 
$\int_{\R} \phi(x) d\mu_n(x)$ converges to $\int_{\R} \phi(x) d\mu(x)$ almost surely.
\end{proof}

Observe that we only needed the convergence of the imaginary part of the Stieltjes transform to control the convergence of the probability measures. This is essentially because the imaginary part of the integral kernel is an approximation of identity. \footnote{Considering the limit of the real part of the Stieltjes transform leads to the theory of Hilbert transform which is not described here.}

\chapter{Deriving the semicircular law}
\label{semic}

The objective is clear now: deriving the almost sure convergence of empirical spectral distributions to a limit reduces to proving the almost sure convergence of the Stieltjes tranforms.
If the almost sure limit of Stieltjes transforms exists, we can deduce the limit measure by taking limit with the limit function.

If we denote the $n$ eigenvalues of $W_n$ by $\lambda_1, \ldots, \lambda_n$,  the Stieltjes transform of the ESD can be written as

\begin{eqnarray*}
S\mu_{\frac{1}{\sqrt{n}}W_n}(z) & = & 
\frac{1}{n} \sum_{i=1}^n \frac{1}{\lambda_i/\sqrt{n} - z}\\
& = & \frac{1}{n} tr(\frac{1}{\sqrt{n}}W_n - z I_n)^{-1}.
\end{eqnarray*}

From now on, $z$ will be a fixed complex number with positive imaginary part. We will prove that the Stieltjes transforms converge almost surely in two parts. First we prove that the expectations of the Stieltjes transforms have a limit function, then we prove that the difference of Stieltjes transform and its expectation goes to zero almost surely.

Consider the expectation of the empirical spectral measure. By linearity of expectation, we have

\begin{eqnarray*}
ES\mu_{\frac{1}{\sqrt{n}}W_n}(z) & = &
E(\frac{1}{n} tr(\frac{1}{\sqrt{n}}W_n - z I_n)^{-1})\\
& = & E(\frac{1}{n} \sum_{l=1}^n(\frac{1}{\sqrt{n}}W_n - z I_n)^{-1}_{ll})\\
& = & \frac{1}{n} \sum_{l=1}^n E(\frac{1}{\sqrt{n}}W_n - z I_n)^{-1}_{ll}\\
& = & E(\frac{1}{\sqrt{n}}W_n - z I_n)^{-1}_{nn}.
\end{eqnarray*}

The last equality holds because by Cramer's rule, the random variables $(1/\sqrt{n} W_n - z I_n)^{-1}_{ll}$ are identically distributed.
Using Schur complement, we have

\begin{equation*}
E(\frac{1}{\sqrt{n}}W_n - z I_n)^{-1}_{nn} = E\frac{1}{-z+\frac{\xi_{nn}}{\sqrt{n}}-\frac{1}{n}C^* (\frac{1}{\sqrt{n}}W_{n-1}-zI_{n-1})^{-1}C},
\end{equation*}

where $C$ is the column vector $(\xi_{in})_{i=1}^{n-1}$.

First, consider the term $\xi_{nn}/\sqrt{n}$. From zero mean and unit variance hypothesis, it immediately follows by Markov's inequality that

\begin{equation*}
P(|\frac{\xi_{nn}}{\sqrt{n}}|>\frac{\lambda}{\sqrt[4]{n}}) \leq \frac{1}{\sqrt{n}\lambda^2}. 
\end{equation*}

This means informally that the term is usually bounded by $o(1)$. Notice that we could have assumed instead that the diagonal elements have a finite mean and variance, and a similar result would have followed.

Now consider the other random term. Notice that the matrix $A = (\frac{1}{\sqrt{n}}W_{n-1}-zI_{n-1})^{-1}$ has $n-1$ eigenvalues, and the eigenvalues are bounded below in absolute value by $|\Im(z)|^{-1}$. Also, matrix $A$ is independent from the vector $C$. We will prove the concentration of the term $C^* A C$ near a deterministic value.

\section{Concentration of the quadratic form $C^* A C$}

The matrix $A$ is independent of the vector $C$. We may therefore consider the conditional expectation with fixed $A$.

\begin{lem}
For a fixed matrix $A$, we have
\begin{equation*}
E(C^* A C) = tr(A).
\end{equation*}
\end{lem}

\begin{proof}
Denote $c_i = \xi_{in}$ for $1 \leq i \leq n-1$.
As the random variables $c_i$ are independent and have mean zero and unit variance, we can compute
\begin{eqnarray*}
E(C^* A C) & = & E(\sum_{i=1}^{n-1} \sum_{j=1}^{n-1} \overline{c_i} a_{ij} c_j)\\
& = & \sum_{i=1}^{n-1} \sum_{j=1}^{n-1} E(\overline{c_i} a_{ij} c_j)\\
& = & \sum_{i=1}^{n-1} (a_{ii} E(\overline{c_i}c_i) + \sum_{j=1,j \neq i}^{n-1} a_{ij} E(\overline{c_i} c_j))\\
& = & \sum_{i=1}^{n-1} (a_{ii} E(|c_i|^2 ) + \sum_{j=1,j \neq i}^{n-1} a_{ij} E(\overline{c_i}) E(c_j))\\
& = & \sum_{i=1}^{n-1} (a_{ii} \cdot 1 + \sum_{j=1,j \neq i}^{n-1} a_{ij} \cdot 0)\\
& = & \sum_{i=1}^{n-1} a_{ii}\\
& = & tr(A).
\end{eqnarray*} 
\end{proof}

\begin{lem}
For a fixed matrix $A$ with operator norm bounded by $c>0$, we have
\begin{equation*}
Var(C^* A C) \leq R \| A \|_2^2 \leq n R c^2.
\end{equation*}
for a constant $R>0$ independent from $A$.
\end{lem}

\begin{proof}
We computed the expectation in the previous lemma. As $Var(Y) = E(|Y|^2)-|E(Y)|^2$, we need to compute $E(|C^* A C|^2)$. Similarly to the previous lemma, we denote the components of $C$ by $c_i$ and compute
\begin{eqnarray*}
E(|C^* A C|^2) & = & E(|\sum_{i=1}^{n-1} \sum_{j=1}^{n-1} \overline{c_i} a_{ij} c_j|^2)\\
& = & E((\sum_{i=1}^{n-1} \sum_{j=1}^{n-1} \overline{c_i} a_{ij} c_j)
(\overline{\sum_{i=1}^{n-1} \sum_{j=1}^{n-1} \overline{c_i} a_{ij} c_j}))\\
& = & E (\sum_{i_1=1}^{n-1}\sum_{i_2=1}^{n-1}
\sum_{j_1=1}^{n-1}\sum_{j_2=1}^{n-1} 
\overline{c_{i_1}} a_{i_1j_1} c_{j_1}c_{i_2} \overline{a_{i_2j_2} c_{j_2}})\\
& = & \sum_{i_1=1}^{n-1}\sum_{i_2=1}^{n-1}
\sum_{j_1=1}^{n-1}\sum_{j_2=1}^{n-1} 
E(\overline{c_{i_1}} a_{i_1j_1} c_{j_1}c_{i_2} \overline{a_{i_2j_2} c_{j_2}}).
\end{eqnarray*}
Now recall that all $c_k$ have mean zero and are independent. This means that if in one term one of the indices $i_1, i_2, j_1$ and $j_2$ is distinct from others, that term vanishes.

The only remaining terms are the ones in one of the following cases:

(1) $i_1 = i_2 = j_1 = j_2$

(2) $i_1=i_2 \neq j_1=j_2$

(3) $i_1 = j_1 \neq i_2 = j_2$

(4) $i_1 = j_2 \neq i_2 = j_1$

Let's go through these cases individually. Denote by $S_i$ the sum of the terms from case $i$.
For case (1), recall that the non-diagonal elements have a finite fourth moment, so $E(|c_{k}|^4) = K < \infty$. Letting $i_1=i_2=j_1=j_2=k$, we have
\begin{eqnarray*}
S_1 & = & \sum_{k=1}^{n-1} E(\overline{c_{k}} a_{kk} c_{k}c_{k} \overline{a_{kk} c_{k}})\\
& = & \sum_{k=1}^{n-1} |a_{kk}|^2 E(|c_{k}|^4)\\
& = & \sum_{k=1}^{n-1} K |a_{kk}|^2
\end{eqnarray*}
In case (2), set $i_1=i_2=k_1$ and $j_1=j_2=k_2$, and we have 
\begin{eqnarray*}
S_2 & = & \sum_{k_1=1}^{n-1} \sum_{k_2=1, k_1\neq k_2}^{n-1} E(\overline{c_{k_1}} a_{k_1k_2} c_{k_2}c_{k_1} \overline{a_{k_1k_2} c_{k_2}})\\
& = & \sum_{k_1=1}^{n-1} \sum_{k_2=1, k_1\neq k_2}^{n-1} |a_{k_1k_2}|^2 E(|c_{k_1}|^2)  E(|c_{k_2}|^2)\\
& = & \sum_{k_1=1}^{n-1} \sum_{k_2=1, k_1\neq k_2}^{n-1} |a_{k_1k_2}|^2
\end{eqnarray*}
In case (3), setting $i_1=j_1=k_1$ and $i_2=j_2=k_2$ gives
\begin{eqnarray*}
S_3 & = & \sum_{k_1=1}^{n-1} \sum_{k_2=1, k_1\neq k_2}^{n-1} E(\overline{c_{k_1}} a_{k_1k_1} c_{k_1}c_{k_2} \overline{a_{k_2k_2} c_{k_2}})\\
& = & \sum_{k_1=1}^{n-1} \sum_{k_2=1, k_1\neq k_2}^{n-1} a_{k_1k_1} \overline{a_{k_2k_2}} E(|c_{k_1}|^2)  E(|c_{k_2}|^2)\\
& = & \sum_{k_1=1}^{n-1} \sum_{k_2=1, k_1\neq k_2}^{n-1} a_{k_1k_1} \overline{a_{k_2k_2}}
\end{eqnarray*}
Notice that the sums in previous cases have been real. For (1) and (2) all the terms are real, and case (3) may be written as
\begin{equation*}
S_3 = \sum_{k_1=1}^{n-1} \sum_{k_2=k_1+1}^{n-1} (a_{k_1k_1} \overline{a_{k_2k_2}}+ \overline{a_{k_1k_1}} a_{k_2k_2}).
\end{equation*}
Finally in case (4), setting $i_1=j_2=k_1$ and $i_2=j_1=k_2$ to compute
\begin{eqnarray*}
S_4 & = & \sum_{k_1=1}^{n-1}\sum_{k_2=1, k_1\neq k_2}^{n-1}
E(\overline{c_{k_1}} a_{k_1k_2} c_{k_2}c_{k_2} \overline{a_{k_2k_1} c_{k_1}})\\
& = & \sum_{k_1=1}^{n-1}\sum_{k_2=1, k_1 \neq k_2}^{n-1} a_{k_1k_2} \overline{a_{k_2k_1}}
E(\overline{c_{k_1}}^2) E( c_{k_2}^2)
\end{eqnarray*}
We know that $S_4$ must be real. Using triangle inequality and arithmetic-geometric mean inequality, we estimate
\begin{eqnarray*}
S_4 & = & \sum_{k_1=1}^{n-1}\sum_{k_2=1, k_1 \neq k_2}^{n-1} a_{k_1k_2} \overline{a_{k_2k_1}}
E(\overline{c_{k_1}}^2) E( c_{k_2}^2)\\
& \leq & \sum_{k_1=1}^{n-1}\sum_{k_2=1, k_1 \neq k_2}^{n-1} |a_{k_1k_2}| |a_{k_2k_1}|
|E(\overline{c_{k_1}}^2)| |E( c_{k_2}^2)|\\
& \leq & \sum_{k_1=1}^{n-1}\sum_{k_2=1, k_1 \neq k_2}^{n-1} |a_{k_1k_2}| |a_{k_2k_1}|
E(|c_{k_1}|^2) E(|c_{k_2}|^2)\\
& = & \sum_{k_1=1}^{n-1}\sum_{k_2=1, k_1 \neq k_2}^{n-1} |a_{k_1k_2}| |a_{k_2k_1}|\\
& \leq & \sum_{k_1=1}^{n-1}\sum_{k_2=1, k_1 \ neq k_2}^{n-1} \frac{|a_{k_1k_2}|^2+|a_{k_2k_1}|^2}{2}\\
& = & \sum_{k_1=1}^{n-1}\sum_{k_2=1, k_1 \neq k_2}^{n-1} |a_{k_1k_2}|^2.
\end{eqnarray*}

Now
\begin{eqnarray*}
Var(C^* A C) & = & S_1+S_2+S_3+S_4 - |E(C^* A C)|^2\\
& = & \sum_{k=1}^{n-1} (K-1) |a_{kk}|^2 + |tr(A)|^2 + S_2+S_4 - |tr(A)|^2\\
& = &  \sum_{k=1}^{n-1} (K-1) |a_{kk}|^2 + \sum_{k_1=1}^{n-1} \sum_{k_2=1, k_1\neq k_2}^{n-1} |a_{k_1k_2}|^2+ S_4\\
& \leq & \sum_{k=1}^{n-1} (K-1) |a_{kk}|^2 + \sum_{k_1=1}^{n-1} \sum_{k_2=1, k_1\neq k_2}^{n-1} 2|a_{k_1k_2}|^2\\
& \leq & \max(K-1,2) \sum_{k_1=1}^{n-1} \sum_{k_2=1}^{n-1} |a_{k_1k_2}|^2\\
& = & R \|A\|_2^2. 
\end{eqnarray*}
Observe that the constant $R = \max(K-1,2)$ does not depend on $n$ or $A$. If the operator norm of $A$ is bounded by $c$, then $|Ae_i|^2\leq c^2$. But this means that
\begin{eqnarray*}
R \|A\|_2^2 & = & R \sum_{k_1=1}^{n-1} \sum_{k_2=1}^{n-1} |a_{k_1k_2}|^2\\
& = & R \sum_{k_2=1}^{n-1} \sum_{k_1=1}^{n-1} |a_{k_1k_2}|^2\\
& = & R \sum_{k_2=1}^{n-1} |Ae_{k_2}|^2\\
& \leq & R n c^2
\end{eqnarray*}
\end{proof}

We can now apply Chebyshev's inequality to conclude $P(|C^* A C - tr(A)|>\lambda) \leq nRc^2/\lambda^2$, or equivalently $P(|C^* A C - tr(A)|>n^{\frac{3}{4}}\lambda) \leq Rc^2/\sqrt{n}\lambda^2$. Undoing the conditioning, we have

\begin{equation*}
P(|\frac{1}{n}C^* A C - \frac{1}{n}tr(A)|>\lambda / \sqrt[4]{n}) \leq Rc^2/\sqrt{n}\lambda^2 .
\end{equation*}

Next, we will approximate the term $\frac{1}{n}tr(\frac{1}{\sqrt{n}}W_{n-1}-zI_{n-1})^{-1}$ by the original Stieltjes transform $S\mu_{\frac{1}{\sqrt{n}}W_n}(z)$.

\begin{lem} \label{est5}
For any $l$, denote $W_n^l$ the matrix $W_n$ with $l$th row and column removed. We have the estimate
\begin{equation*}
|\frac{1}{n}tr((\frac{1}{\sqrt{n}}W_n^l-zI_{n-1})^{-1}) - S_{\mu_{\frac{1}{\sqrt{n}}W_n}}(z)| \leq \frac{C_z}{n}.
\end{equation*}
\end{lem}

\begin{proof}
Knowing that both $W_n$ and $W_n^l$ are hermitian matrices, let $\lambda_1 \leq \lambda_2 \leq \ldots \lambda_n$ be the eigenvalues of $W_n$ and $\lambda_1' \leq \lambda_2' \leq \ldots \lambda_{n-1}'$ be the eigenvalues of $W_n^l$. We know by Cauchy interlacing formula that

\begin{equation*}
\lambda_{i} \leq \lambda_{i}' \leq \lambda_{i+1} (A)
\end{equation*}

for $1\leq i \leq n-1$.

Now we see that the eigenvalues of the matrix $B = (\frac{1}{\sqrt{n}}W_n^l-zI_{n-1})^{-1}$ are $(\lambda_i'/\sqrt{n}-z)^{-1}$ for $1\leq i \leq n-1$. Rewriting the difference we want to estimate and using triangle inequality gives

\begin{eqnarray*}
\left|\frac{1}{n}tr(B) - S_{\mu_{\frac{1}{\sqrt{n}}W_n}}(z)\right| & = & \frac{1}{n}
\left|\sum_{i=1}^{n-1} \frac{1}{\frac{\lambda_i'}{\sqrt{n}}-z} - \sum_{i=1}^{n} \frac{1}{\frac{\lambda_i}{\sqrt{n}}-z}\right|\\
& \leq & \frac{1}{n} \left(
\sum_{i=1}^{n-1} \left|\frac{1}{\frac{\lambda_i'}{\sqrt{n}}-z} - \frac{1}{\frac{\lambda_i}{\sqrt{n}}-z}\right| + \left|\frac{1}{\frac{\lambda_n}{\sqrt{n}}-z}\right| \right).
\end{eqnarray*}

Now we note that the intervals $]\lambda_i/\sqrt{n},\lambda_i'/\sqrt{n}[$ are disjoint by Cauchy interlacing formula. For any sequence $x_1 \leq x_2 \leq ... \leq x_{2m-1} \leq x_{2m}$, we can estimate with triangle inequality

\begin{eqnarray*}
\sum_{j=1}^{m} \left|\frac{1}{x_{2j}-z} - \frac{1}{x_{2j-1}-z}\right| & = &
\sum_{j=1}^{m} \left|\frac{1}{x_{2j}-(a+ib)} - \frac{1}{x_{2j-1}-(a+ib)}\right|\\
& = & \sum_{j=1}^{m} \left|\frac{x_{2j}-a+ib}{x_{2j}-(a+ib)} - \frac{x_{2j-1}-a+ib}{(x_{2j-1}-a)^2+b^2)}\right|\\
& \leq & \sum_{j=1}^{m} \left|\frac{x_{2j}-a}{(x_{2j}-a)^2+b^2} - \frac{x_{2j-1}-a}{(x_{2j-1}-a)^2+b^2)}\right| \\
&  &+ \sum_{j=1}^{m} \left|\frac{b}{(x_{2j}-a)^2+b^2} - \frac{b}{(x_{2j-1}-a)^2+b^2)}\right|.
\end{eqnarray*}

Now we see that function $x \to (x-a)/((x-a)^2+b^2)$ is continuously differentiable. It has a maximum $1/2b$ at the point $x=a+b$, a minimum $-1/2b$ at $x=a-b$ and it approaches zero at $\pm \infty$. This means that the first sum is at most $2/b$.

Likewise, considering the derivative of the function $x \to b/((x-a)^2+b^2)$ we see that it has a maximum $1/b$ at the point $x=a$ and it approaches zero at $\pm \infty$, so the second sum is at most $2/b$.

In other words, the function $x \to (x-z)^{-1}$ has bounded variation for a fixed $z$. Using this for points, we see that  

\begin{eqnarray*}
\left|\frac{1}{n}tr(\frac{1}{\sqrt{n}}W_n^l-zI_{n-1})^{-1} - S_{\mu_{\frac{1}{\sqrt{n}}W_n}}(z)\right| 
& \leq & \frac{1}{n}
\sum_{i=1}^{n-1} \left|\frac{1}{\frac{\lambda_i'}{\sqrt{n}}-z} - \frac{1}{\frac{\lambda_i}{\sqrt{n}}-z}\right| + \left|\frac{1}{\frac{\lambda_n}{\sqrt{n}}-z}\right|\\
& \leq & \frac{1}{n} \left(\frac{2}{b} + \frac{2}{b}  + \left|\frac{1}{\frac{\lambda_n}{\sqrt{n}}-z}\right|\right)\\
& \leq &  \frac{1}{n} \left( \frac{4}{b} + \frac{1}{b} \right)\\
& = & \frac{5}{nb}.
\end{eqnarray*}

We get the claim by selecting $C_z = 5/b$.
\end{proof}

Finally, the following special case of \emph{McDiarmid's inequality}\cite{McD} allows us to estimate the Stieltjes transform by its expectation:

\begin{lem}
For any index $n$ and positive number $\lambda$, we have
\begin{equation*}
P(|S\mu_{\frac{1}{\sqrt{n}}W_n}(z)- ES\mu_{\frac{1}{\sqrt{n}}W_n}(z)|>\lambda) \leq Ce^{-cn\lambda^2}.
\end{equation*}
for some constants $C, c > 0$, depending only on $z$.
\end{lem}

\begin{proof}
We define the real random variables $D_k$ for $0\leq k \leq n$ to be the conditional expectation
\begin{equation*}
D_k = \Re(E(S\mu_{\frac{1}{\sqrt{n}}W_n}(z)|(\xi_{ij})_{i \leq j\leq k}),
\end{equation*}
in other words, the conditional expectation of the real part of the Stieltjes transform when we fix the random variables $\xi_{ij}$ for $i,j$ at most $k$. \footnote{This construction is known as the Doob martingale \cite{Doob}. The argument we will do is essentially using Azuma's inequality \cite{Azu} for this sequence.} We observe that $D_0 = \Re(ES\mu_{\frac{1}{\sqrt{n}}W_n}(z))$ and $D_n = \Re(S\mu_{\frac{1}{\sqrt{n}}W_n}(z))$.

We claim that the inequality $|D_k-D_{k-1}|\leq C_z/n$ holds surely. To see this, note that
\begin{equation*}
D_k-D_{k-1} = \Re E(S\mu_{\frac{1}{\sqrt{n}}W_n}(z) - E(S\mu_{\frac{1}{\sqrt{n}}W_n}(z) | (\xi_{ij})_{i \leq j \neq k}) |(\xi_{ij})_{i \leq j\leq k}).
\end{equation*}

But we had in \ref{est5} that surely 
\begin{equation*}
|\frac{1}{n}tr((\frac{1}{\sqrt{n}}W_n^k-zI_{n-1})^{-1}) - S_{\mu_{\frac{1}{\sqrt{n}}W_n}}(z)| \leq \frac{C_z}{n},
\end{equation*}
so $|S\mu_{\frac{1}{\sqrt{n}}W_n}(z) - E(S\mu_{\frac{1}{\sqrt{n}}W_n}(z)| (\xi_{ik})_{i\leq k})|$ is bounded above by $C_z/n$. Taking the conditional expectation gives $|D_k-D_{k-1}|\leq C_z/n$.

Next, we consider the exponential moment $Ee^{tD_n}$.
We are going to prove next that
\begin{equation*}
Ee^{tD_k} \leq e^{\frac{1}{8}t^2\left(\frac{C_z}{n}\right)^2} Ee^{tD_k-1}
\end{equation*}
for any $1\leq k \leq n$. Applied inductively, this gives
\begin{equation*}
Ee^{tD_n} \leq e^{\frac{C_z^2t^2}{8n}} Ee^{tD_0}.
\end{equation*}

As $D_{k-1}$ does not depend on $\xi_{ik}$ for $i\leq k$, conditioning gives
\begin{eqnarray*}
Ee^{tD_k} & = & EE(e^{tD_k}|(\xi_{ij})_{i\leq j \neq k})\\
& = & EE(e^{t(D_k-D_{k-1})}e^{tD_{k-1}}|(\xi_{ij})_{i\leq j \neq k})\\
& = & E(e^{tD_{k-1}}E(e^{t(D_k-D_{k-1})}|(\xi_{ij})_{i\leq j \neq k})).
\end{eqnarray*}

Using Hoeffding's lemma, we conclude
\begin{equation*}
E(e^{t(D_k-D_{k-1})}|(\xi_{ij})_{i\leq j \neq k}) \leq
e^{\frac{1}{8}t^2 \left(\frac{C_z}{n}\right)^2},
\end{equation*}

so we have

\begin{equation*}
Ee^{tD_k} = E(e^{tD_{k-1}}E(e^{t(D_k-D_{k-1})}|(\xi_{ij})_{i\leq j \neq k})) \leq e^{\frac{1}{8}t^2 \left(\frac{C_z}{n}\right)^2}Ee^{tD_{k-1}}. 
\end{equation*}

Now we have 

\begin{equation*}
Ee^{t\Re(S\mu_{\frac{1}{\sqrt{n}}W_n}(z))} \leq e^{\frac{C_z^2t^2}{8n}} e^{tE\Re(S\mu_{\frac{1}{\sqrt{n}}W_n}(z))},
\end{equation*}

or equivalently

\begin{equation*}
Ee^{t(\Re(S\mu_{\frac{1}{\sqrt{n}}W_n}(z))-E\Re(S\mu_{\frac{1}{\sqrt{n}}W_n}(z)))} \leq e^{\frac{C_z^2t^2}{8n}}.
\end{equation*}

Now by Markov's inequality for any $a > 0,$ we have

\begin{equation*}
P\left(e^{t(\Re(S\mu_{\frac{1}{\sqrt{n}}W_n}(z))-E\Re(S\mu_{\frac{1}{\sqrt{n}}W_n}(z)))} \geq  a \right) \leq \frac{e^{\frac{C_z^2t^2}{8n}}}{a},
\end{equation*}

which we may rewrite setting $a = e^{t\lambda}$ as

\begin{equation*}
P\left((\Re(S\mu_{\frac{1}{\sqrt{n}}W_n}(z))-E\Re(S\mu_{\frac{1}{\sqrt{n}}W_n}(z))) \geq  \lambda \right) \leq \frac{e^{\frac{C_z^2t^2}{8n}}}{e^{t\lambda}}.
\end{equation*}

The right-hand side has a minimum at $t=(4\lambda n)/(C_z^2)$, so we get

\begin{equation*}
P\left((\Re(S\mu_{\frac{1}{\sqrt{n}}W_n}(z))-E\Re(S\mu_{\frac{1}{\sqrt{n}}W_n}(z))) \geq  \lambda \right) \leq
e^{\frac{-2n\lambda^2}{C_z^2}}.
\end{equation*}

If we do the same argument with random variables $-D_k$, we obtain

\begin{equation*}
P\left((\Re(S\mu_{\frac{1}{\sqrt{n}}W_n}(z))-E\Re(S\mu_{\frac{1}{\sqrt{n}}W_n}(z))) \leq  -\lambda \right) \leq
e^{\frac{-2n\lambda^2}{C_z^2}}.
\end{equation*}

Taken together, these two estimates imply that

\begin{equation*}
P\left(|\Re(S\mu_{\frac{1}{\sqrt{n}}W_n}(z)-ES\mu_{\frac{1}{\sqrt{n}}W_n}(z))| \geq  \lambda \right)  \leq 2
e^{\frac{-2n\lambda^2}{C_z^2}}.
\end{equation*}

We can repeat this argument for the imaginary part to conclude

\begin{equation*}
P\left(|\Im(S\mu_{\frac{1}{\sqrt{n}}W_n}(z)-ES\mu_{\frac{1}{\sqrt{n}}W_n}(z))| \geq  \lambda \right)  \leq 2
e^{\frac{-2n\lambda^2}{C_z^2}}.
\end{equation*}

This means that we have an upper bound

\begin{eqnarray*}
P(|S\mu_{\frac{1}{\sqrt{n}}W_n}(z)- ES\mu_{\frac{1}{\sqrt{n}}W_n}(z)|>\lambda) & \leq &
P(|\Re(S\mu_{\frac{1}{\sqrt{n}}W_n}(z)- ES\mu_{\frac{1}{\sqrt{n}}W_n}(z))|>\lambda/2) \\ & &+ 
P(|\Im(S\mu_{\frac{1}{\sqrt{n}}W_n}(z)- ES\mu_{\frac{1}{\sqrt{n}}W_n}(z))|>\lambda/2)\\
& \leq & 2
e^{\frac{-n\lambda^2}{2C_z^2}} + 2
e^{\frac{-n\lambda^2}{2C_z^2}}\\
& = & 4
e^{\frac{-n\lambda^2}{2C_z^2}}.
\end{eqnarray*}

\end{proof}

By triangle inequality, we can take all of the above to conclude
\begin{eqnarray*}
P(|\frac{1}{n}C^* A C - ES\mu_{\frac{1}{\sqrt{n}}W_n}(z)| \geq 3 /\sqrt[4]{n}) & \leq &
P(|\frac{1}{n}C^* A C - \frac{1}{n}tr(A)| \geq 1/\sqrt[4]{n})\\
& & + P(|\frac{1}{n}tr(A) - S\mu_{\frac{1}{\sqrt{n}}W_n}(z)| \geq 1/\sqrt[4]{n})\\
& & + P(|S\mu_{\frac{1}{\sqrt{n}}W_n}(z) - ES\mu_{\frac{1}{\sqrt{n}}W_n}(z)| \geq 1/\sqrt[4]{n})\\
& \leq &
\frac{Rc^2}{\sqrt{n}}+0+ Ce^{-c\sqrt{n}}
\end{eqnarray*}
for any $n$ such that $1/\sqrt[4]{n} > C_z/n$, meaning $n>C_z^{\frac{3}{4}}$.

\section{Finishing the computation}

Setting $r(n) = 4/\sqrt[4]{n}$ and $p(n)= (1+R(\Im(z))^{-2})/\sqrt{n} + Ce^{-c\sqrt{n}}$, we have that

\begin{equation*}
ES\mu_{\frac{1}{\sqrt{n}}W_n}(z) = (1-p(n)) \frac{-1}{z+ES\mu_{\frac{1}{n}W_n}(z) + \eps{n}} + p(n)t(n),
\end{equation*}

where for the error terms we have $|\eps(n)|\leq r(n)$ and $|t(n)| \leq 1/\Im(z)$. Notice that both $r(n)$ and $p(n)$ converge to zero as $n$ goes to infinity.


By continuity (as the denominator is bounded away from zero) 

\begin{equation*}
ES\mu_{\frac{1}{\sqrt{n}}W_n}(z) = -\frac{1}{z+ES\mu_{\frac{1}{n}W_n}(z)} + o(1).
\end{equation*}

Solving this, we get
\begin{equation*}
ES\mu_{\frac{1}{\sqrt{n}}W_n}(z) = \frac{-z \pm \sqrt{z^2 - 4}}{2} +o(1).
\end{equation*}

Since $S\mu$ maps the upper half-plane to the upper half-plane,  

\begin{equation*}
ES\mu_{\frac{1}{\sqrt{n}}W_n}(z) = \frac{-z + \sqrt{z^2 - 4}}{2} +o(1),
\end{equation*}

where the branch of the square root has positive imaginary part in upper half-plane.

We know now that the expectations of the Stieltjes transforms converge to a limit function. Recall that we proved the following estimate:

\begin{equation*}
P(|S\mu_{\frac{1}{\sqrt{n}}W_n}(z)- ES\mu_{\frac{1}{\sqrt{n}}W_n}(z)|>\lambda) \leq Ce^{-cn\lambda^2}.
\end{equation*} 

Now, for any $\eps>0$, the sum $\sum_{n=1}^{\infty} Ce^{-cn\varepsilon}$ converges as a geometric series. Combining previous estimate with Borel-Cantelli lemma implies that almost surely

\begin{equation*}
S\mu_{\frac{1}{\sqrt{n}}W_n}(z) \to g(z) = \frac{-z + \sqrt{z^2-4}}{2}.
\end{equation*}

Now investigating the behaviour of imaginary part near real axis has

\begin{eqnarray*}
\frac{1}{\pi} \Im(g(x+yi)) & = &
\frac{y}{2\pi} + \frac{\sqrt{\frac{4-x^2+y^2+\sqrt{(x^2-y^2-4)^2+4x^2y^2}}{2}}}{2\pi}\\
& \to & \frac{1}{2\pi} \sqrt{(4-x^2)_+},
\end{eqnarray*}

meaning that the limit of the imaginary part of the function is the \emph{Wigner semicircular distribution}. Now that we know what the limiting probability measure should be, the problem of verifying the limit measure reduces to straightforward computation.

\begin{theo}
Let $\mu_{sc}$ be the probability measure of the Wigner semicircular distribution. Then
\begin{equation*}
S\mu_{sc}(z) = g(z).
\end{equation*}
\end{theo}

\begin{proof}
We will compute the Stieltjes transform at point $z$ in upper half-plane. First, making the substitution $x = 2 \cos \theta$, we get
\begin{eqnarray*}
S\mu_{sc}(z) & = & \int_{\R} \frac{1}{x-z} d\mu_{sc}(x)\\
& = & \frac{1}{2\pi} \int_{-2}^2 \frac{\sqrt{4-x^2}}{x-z} dx\\
& = & \frac{1}{2\pi} \int_{0}^{\pi} \frac{\sqrt{4-4\cos^2\theta}}{2\cos\theta -z} 2 \sin\theta d\theta\\
& = & \frac{1}{2\pi} \int_{0}^{\pi} \frac{4\sin^2\theta}{2\cos\theta -z} d\theta\\
& = & \frac{1}{4\pi} \int_{0}^{2\pi} \frac{4\sin^2\theta}{2\cos\theta -z} d\theta.
\end{eqnarray*}

Expressing the sine and cosine via complex exponential function and making substitution $e^{i\theta} = w$, we get
\begin{eqnarray*}
S\mu_{sc}(z) & = & \frac{1}{4\pi} \int_{0}^{2\pi} \frac{4\sin^2\theta}{2\cos\theta -z} d\theta\\
& = & \frac{1}{4\pi} \int_{0}^{2\pi} \frac{4(\frac{1}{2i}(e^{i\theta}-e^{-i\theta}))^2}{2(\frac{1}{2}(e^{i\theta}+e^{-i\theta})) -z} d\theta\\
& = & \frac{1}{4\pi} \int_{0}^{2\pi} \frac{-(e^{2i\theta}-2+e^{-2i\theta})}{(e^{i\theta}+e^{-i\theta}) -z} d\theta\\
& = & \frac{1}{4\pi} \int_{|w|=1} \frac{2-w^2-w^{-2}}{(w+w^{-1}) -z} \frac{dw}{iw}\\
& = & \frac{1}{4\pi} \int_{|w|=1} \frac{2-w^2-w^{-2}}{(w+w^{-1}) -z} \frac{dw}{iw}\\
& = & \frac{1}{4\pi i} \int_{|w|=1} \frac{2-w^2-w^{-2}}{w^2+1 -zw} dw\\
& = & \frac{1}{4\pi i} \int_{|w|=1} \frac{-w^4+2w^2-1}{w^2(w^2+1 -zw)} dw\\
& = & \frac{1}{4\pi i} \int_{|w|=1} h_z(w) dw.
\end{eqnarray*}

We are going to apply the residue theorem to compute the integral. As the function $h_z$ has poles at $w=0$ and $w=\frac{z \pm \sqrt{z^2-4}}{2}$, we need to figure out if the poles at $w=\frac{z \pm \sqrt{z^2-4}}{2}$ lie in the unit disk.

Denote those two poles by $p_1$ and $p_2$. Then, $p_1 + p_2 = z$ and $p_1p_2=1$, which implies that exactly one of them lies in the unit disk. Since $z$ has positive imaginary part, the one with smaller absolute value must have negative imaginary part.
Therefore, the pole $w=\frac{z - \sqrt{z^2-4}}{2} = p_1$ lies in the unit disk, where the square root branch has positive imaginary part.

Calculating the residues is straightforward. For $w=0$:

\begin{eqnarray*}
Res(h_z,0) & = &
\left(\frac{d}{dw} \frac{-w^4+2w^2-1}{w^2-zw+1} \right)_{w=0}\\
& = & \left(\frac{(-4w^3+4w)(w^2-zw+1)-(-w^4+2w^2-1)(2w-z)}{(w^2-zw+1)^2} \right)_{w=0}\\
& = & \frac{(-4\cdot 0^3+4 \cdot 0)(0^2-z\cdot 0+1)-(-0^4+2\cdot 0^2-1)(2\cdot 0-z)}{(0^2-z\cdot 0+1)^2} \\
& = & -z
\end{eqnarray*}

For $w=p_1$, we use the fact that $p_1p_2 = 1$:

\begin{eqnarray*}
Res(h_z,p_1) & = &
\left(\frac{-w^4+2w^2-1}{w^2(w-p_2)} \right)_{w=p_1}\\
& = & \frac{-p_1^4+2p_1^2-1}{p_1^2(p_1-p_2)}\\
& = & \frac{-(p_1^2-1)^2}{p_1^2(p_1-p_2)}\\
& = & \frac{-(p_1^2-p_1p_2)^2}{p_1^2(p_1-p_2)}\\
& = & \frac{-p_1^2(p_1-p_2)^2}{p_1^2(p_1-p_2)}\\
& = & p_2-p_1\\
& = & \frac{z + \sqrt{z^2-4}}{2} - \frac{z - \sqrt{z^2-4}}{2}\\
& = & \sqrt{z^2-4}.
\end{eqnarray*}

Now by residue theorem we have

\begin{eqnarray*}
S\mu_{sc}(z) & = & \frac{1}{4\pi i} \int_{|w|=1} \frac{-w^4+2w^2-1}{w^2(w^2+1 -zw)} dw\\
& = & \frac{1}{2} ( -z + \sqrt{z^2-4}).
\end{eqnarray*}

\end{proof}

By Stieltjes continuity theorem, we deduce that $\mu_{\frac{1}{\sqrt{n}}W_n}$ converges to the semicircular distribution almost surely in vague topology. We have finally proved:

\begin{theo}
\emph{Wigner semicircular law.} For an ensemble of Wigner matrices $(W_n)_{n=1}^{\infty}$ with all elements of matrices being independent and identically distributed, having zero mean, unit variance and bounded fourth moment, the empirical spectral distributions $\mu_{\frac{1}{\sqrt{n}}W_n}$ converges almost surely to Wigner semicircular distribution $\mu_{sc}$.
\end{theo}


\chapter{What next?}
\label{future}

Recall that the central limit theorem states that for a sequence of independent identically distributed real random variables $(X_i)$ with zero mean and unit variance, the distribution of $(\sum_{i=1}^{n}X_i)/\sqrt{n}$ approaches the standard normal distribution. As the sum of independent Gaussian random variables is Gaussian, this can be used in a proof of the central limit theorem. However, this approach does not work for empirical spectral distributions because of the following lemma:

\begin{lem}
For any $K>0$, we have for sufficiently large $n$

\begin{equation*}
P(\mu_{\frac{1}{\sqrt{n}}W_n} (\{x \in \R : |x|>K\} > 0)) > 0.
\end{equation*}

\end{lem}

\begin{proof}
By unit variance hypothesis, we know that $P(|\xi_{ij}| \geq 1) > 0$ for any $i<j$.
By triangle inequality, either $P(|\Re(\xi_{ij})| \geq 1/2) > 0$ or $P(|\Im(\xi_{ij})| \geq 1/2) > 0$.

Suppose that $P(|\Re(\xi_{ij})| \geq 1/2) > 0$.
By considering the matrix $-W_n$ we may assume $P(\Re(\xi_{ij}) \geq 1/2) = p > 0$. Define $V=\{z \in \C : \Re(z)> 1/2\}$.

Considering the projections to the real line, we see that if $z_1,\ldots,z_n \in V$, then $|\sum_{j=1}^{n}z_j| \geq n/2$. Since $V$ is symmetric with respect to the real axis, we have that $\xi_{ij} \in V$ if and only if $\xi_{ji} \in V$.

For any $n>1$, we have that $P(\xi_{ij} \in V, 1 \leq i<j\leq n) = p^{n(n-1)/2} > 0$. Additionally, by unit variance hypothesis, we know that $P(|\xi_{ii}|<2, 1\leq i \leq n) = q^n > 0$. When both of these events happen, consider the vector $ v =e_1+e_2+\ldots + e_n$ for $n > 5$:

\begin{eqnarray*}
\|\frac{1}{\sqrt{n}}W_nv\| & = & \frac{1}{\sqrt{n}} \left\| \sum_{i=1}^n (\sum_{j=1}^n\xi_{ij})e_i \right\|  \\
& = & \frac{1}{\sqrt{n}} \sqrt{\sum_{i=1}^n |\sum_{j=1}^n\xi_{ij}|^2}\\
& \geq & \frac{1}{\sqrt{n}} \sqrt{\sum_{i=1}^n \left(|\sum_{j=1, j\neq i}^n \xi_{ij}|-|\xi_{ii}|\right)^2}\\
& \geq & \frac{1}{\sqrt{n}} \sqrt{\sum_{i=1}^n \left(|\sum_{j=1, j\neq i}^n \xi_{ij}|-2\right)^2}\\
& \geq &  \frac{1}{\sqrt{n}} \sqrt{\sum_{i=1}^n \left(\frac{n-1}{2}-2\right)^2}\\
& = & \frac{1}{\sqrt{n}} \sqrt{n \left(\frac{n-1}{r}-2\right)^2}\\
& = & \sqrt{\left(\frac{n-1}{2}-2\right)^2}\\
& = & \frac{n-5}{2}.
\end{eqnarray*}

Now $\|v\| = \sqrt{n}$, so we know that the operator norm of $\frac{1}{\sqrt{n}} W_n$ is at least $((n-5)/2\sqrt{n}$. For large enough $n$ this is always bigger than $K$. This means that $\mu_{\frac{1}{\sqrt{n}}W_n} \{x \in \R : |x|>K\} > 0)) \geq 1/n > 0$ with probability at least $p^{n(n-1)/2}q^n$.


Now we consider the case $P(|\Im(\xi_{ij})| \geq 1/2) > 0$. We may assume that $P(\Im(\xi_{ij}) \geq 1/2) > 0$. Representing the half-plane $V = \{ z \in \C : \Im(z) \geq 1/2\}$ as a countable union, we conclude that $P(\Im(\xi_{ij} \in [r,2r])) = p > 0$ for some $r\geq 1/2$.

Defining $v$ as before, we have 

\begin{equation*}
\|\frac{1}{\sqrt{n}}W_nv\| = \frac{1}{\sqrt{n}} \sqrt{\sum_{i=1}^n |\sum_{j=1}^n\xi_{ij}|^2}.
\end{equation*}

Now we know that $\xi_{ij}$ is purely real if $i=j$. Then we know

\begin{eqnarray*}
\frac{1}{\sqrt{n}} \sqrt{\sum_{i=1}^n |\sum_{j=1}^n\xi_{ij}|^2} & = & \frac{1}{\sqrt{n}} \sqrt{\sum_{i=1}^n |\sum_{j=1,j\neq i}^n \Im(\xi_{ij})|^2 + |\sum_{j=1}^{n} \Re(\xi_{ij})|^2}\\
& \geq & \frac{1}{\sqrt{n}} \sqrt{\sum_{i=1}^n |\sum_{j=1,j\neq i}^n\Im(\xi_{ij})|^2}
\end{eqnarray*}

We know that $P(\Im(\xi_{ij} \in [r,2r], 1 \leq i<j\leq n) = p^{n(n-1)/2} > 0$. In the case where this happens we can estimate the inner sums as follows:

\begin{eqnarray*}
|\sum_{j=1,j\neq i}^n \Im(\xi_{ij})| & \geq &
\sum_{j=1,j\neq i}^n \Im(\xi_{ij})\\
& = & \sum_{j=1}^{i-1}Im(\xi_{ij}) + \sum_{j=i+1}^n \Im(\xi_{ij})\\
& \geq & \sum_{j=1}^{i-1} r - \sum_{j=i+1}^n 2r\\
& = & (i-1)r - 2(n-i)r\\
& = & (3i - (2n+1))r
\end{eqnarray*}

We observe that this estimate is positive for $i> (2n+1)/3$. This means we can estimate for $n>7$

\begin{eqnarray*}
\frac{1}{\sqrt{n}} \sqrt{\sum_{i=1}^n |\sum_{j=1,j\neq i}^n \Im(\xi_{ij})|^2} & \geq & \frac{1}{\sqrt{n}} \sqrt{\sum_{i>(2n+1)/3}^n |\sum_{j=1,j\neq i}^n \Im(\xi_{ij})|^2}\\
& \geq & \frac{1}{\sqrt{n}} \sqrt{\sum_{i>(2n+1)/3}^n (3i - (2n+1))^2r^2}\\
& \geq & \frac{r}{\sqrt{n}} \sqrt{\int_{(2n+1)/3}^{n-2} (3x - (2n+1))^2dx}\\
& = & \frac{r}{\sqrt{n}} \sqrt{\frac{1}{9}n^3 - \frac{7}{3}n^2+\frac{49}{3}n-\frac{343}{9}}\\
& = & \frac{r}{\sqrt{n}} \sqrt{\frac{(n-7)^3}{9}}\\
& \geq & \sqrt{\frac{(n-7)^3}{36n}}
\end{eqnarray*}
Now once again, we know that the operator norm must be at least $\sqrt{(n-7)^3}/6n$, so the claim follows for large enough $n$.
\end{proof}

The previous lemma means that for a Wigner matrix $W_n$, the expected proportion of the eigenvalues $\lambda$ with $|\lambda| > K\sqrt{n}$ becomes positive after $n$ goes large enough. By semicircular law, this expectation goes to zero as $n$ goes to infinity for $K > 2$. As the semicircular distribution is compactly supported, the following corollary is evident.

\begin{cor}
It is not possible to choose the random variables $\xi_{ij}$ in Wigner matrix in a way such that for all $n$

\begin{equation*}
E\mu_{\frac{1}{\sqrt{n}}W_n} = \mu_{sc}. 
\end{equation*}

In fact, for any choice of the random variables, the equality does not hold for any $n>C$ for some constant $C$ independent from the choice.

\end{cor}



Having proved the semicircular law, we observe some consequences. For example, we may immediately prove a lower bound for the operator norm of Wigner matrices. 

\begin{theo}
\emph{Lower Bai-Yin theorem}. For an ensemble of Wigner matrices $(W_n)_{n \in \Z_+}$, we have for any $\eps > 0$ almost surely

\begin{equation*}
\|W_n\| > (2-\eps) \sqrt{n}
\end{equation*}

for large enough $n$. Here we denote the operator norm of matrix $A$ with $\| A \|$.
\end{theo}

\begin{proof}
By semicircular law, we know that the empirical spectral distribution of $(1/\sqrt{n}) W_n$ converges to the semicircular distribution almost surely.

Fix $\eps>0$. Let $g : \R \to [0,1]$ be a continuous function with
$g(x)= 1$ if $|x|<2-\eps$ and $g(x) = 0$ if $|x|>2-\eps/2$.

If $\|W_n\| < (2-\eps)\sqrt{n}$, then 

\begin{equation*}
\int_{\R} g(x) d\mu_{\frac{1}{\sqrt{n}}W_n}(x) = 1.
\end{equation*}

But we also see that

\begin{equation*}
\int_{\R} g(x) d\mu_{sc}(x) < 1.
\end{equation*}

This means that we can almost surely find $N$ such that

\begin{equation*}
\| W_n \| > (2-\eps) \sqrt{n}
\end{equation*}

for any $n>N$.
\end{proof}

As was mentioned before, it is also possible to prove the semicircular law by considering the moments of the empirical spectral distribution. Let us prove a slightly modified version of the semicircular law by this method. This also gives an upper bound for the operator norm for the kind of ensembles considered, showing that the previous bound is tight.

\begin{theo}
Let $(\xi_{ij})_{i,j \in \Z_+}$ be random variables (not necessarily identically distributed) such that $\xi_{ij}$
are independent with mean zero and unit variance for $i \leq j$, with diagonal $\xi_{ii}$ real random variables, and $\xi_{ij}=\overline{\xi_{ji}}$ for $i>j$. Assume also that $|\xi_{ij}|<M$ almost surely for some absolute constant $M$, in other words the random variables are uniformly bounded almost certainly.
Then, defining Wigner matrix $W_n$ as before, the empirical spectral distributions $\mu_{\frac{1}{\sqrt{n}}W_n}$ converge to $\mu_{sc}$ almost certainly.
\end{theo}

The following result reduces this claim in a way similar to Stieltjes continuity theorem:

\begin{theo}
\emph{Carleman continuity theorem} Suppose we have
\end{theo}

\begin{theo}
The even moments of the semicircular distribution are

\end{theo}



\appendix

\chapter{Some theorems from probability theory}

Let $X_n$ be random variables for $n \in \N$, and $X$ be another random variable. We say that $X_n$ converges to $X$ almost surely if for any $\eps > 0$

\begin{equation*}
P(|X_n-X|>\eps \textrm{ for infinitely many $n$}) = 0,
\end{equation*}

or equivalently,
\begin{equation*}
P(\limsup_{n \to \infty} |X_n-X|>\eps) =
\lim_{n\to \infty} P(\bigcup_{k\geq n} \{|X_k-X|>\eps\}) = 0.
\end{equation*}


\begin{theo}
Borel-Cantelli lemma, strong version. Suppose that we have sequences of random variables $(X_n)$ and $(Y_n)$. Suppose also that $Y_n$ converges to a random variable $Y$ almost surely, and that $ \sum_n P(|X_n-Y_n|>\eps) < \infty$ for any $\eps > 0$. Then $X_n$ converges to $Y$ almost surely. 
\end{theo}

\begin{proof}
Fix $\eps > 0$. By triangle inequality, we know that for any $n$ we have

\begin{equation*}
\{|X_n-Y|>\eps\} \subset \{|X_n-Y_n|>\eps /2\} \cup \{|Y_n-Y|>\eps /2\}.
\end{equation*}

Using this with union bound, we conclude that 
\begin{equation*}
P(\bigcup_{k\geq n}\{|X_k-Y|>\eps\}) \leq P(\bigcup_{k\geq n}\{|X_k-Y_k|>\eps /2\}) + P(\bigcup_{k\geq n}\{|Y_k-Y|>\eps /2\}).
\end{equation*}

As $Y_n$ converges to $Y$ almost surely, we already know that 
\begin{equation*}
\lim_{n \to \infty} P(\bigcup_{k\geq n}\{|Y_k-Y|>\eps /2\}) = 0.
\end{equation*}

Using sub-additivity, we obtain
\begin{equation*}
P(\bigcup_{k\geq n}\{|X_k-Y_k|>\eps /2\}) \leq \sum_{k\geq n} P(\{|X_k-Y_k|>\eps /2\}),
\end{equation*}
and the right hand side tends to zero as $n$ grows to infinity. This proves that $X_n$ converges to $Y$ almost surely.
\end{proof}

\begin{theo}
Hoeffiding's lemma \cite{Hoef}. Let $X$ be an almost surely bounded real random variable; in other words $X \in [a,b]$ almost surely. Then for any $t \geq 0$
\begin{equation*}
Ee^{tX} \leq e^{tEX} \exp(\frac{(b-a)^2}{8}t^2).
\end{equation*}
\end{theo}

\begin{proof}
We may assume that $EX = 0$ and $b-a = 1$. Fix $t \geq 0$.
An application of Jensen's inequality shows that for any $x \in [a,b]$

\begin{equation*}
e^{tx} \leq (b-x)e^{ta}+(x-a)e^{tb}.
\end{equation*}

This implies that
\begin{eqnarray*}
Ee^{tX} & \leq & (b-EX)e^{ta}+(EX-a)e^{tb}\\
& = & e^{ta}(b-ae^t)\\
& = & e^{ta+\ln(1+a-ae^t)}\\
& = & e^{g(t)}.
\end{eqnarray*}

It remains to show that $g(t) \leq \frac{1}{8}t^2$. Computing the derivatives up to second order and using Taylor's theorem gives
\begin{eqnarray*}
g'(t) & = & a + \frac{-ae^t}{1+a-ae^t}\\
g''(t) & = & -\left( \frac{-ae^t}{1+a-ae^t} \right)^2 + \frac{-ae^t}{1+a-ae^t} \leq \frac{1}{4}\\
g(0) & = &  0\\
g'(0) & = & 0\\
g(t) & = & 0 + 0\cdot t + \frac{g''(\xi)}{2}t^2
\end{eqnarray*}
for some $\xi \in (0,t)$. As $g''(\xi) \leq 1/4$, we obtain $g(t) \leq \frac{1}{8}t^2$, proving the claim.
\end{proof}



\chapter{Hermitian matrices}

An $n \times n$ complex matrix $A$ is a Hermitian matrix if $a_{ij} = \overline{a_{ji}}$ for any $1\leq i,j \leq n$. This is equivalent to having $\langle v, Aw \rangle = \langle Av, w \rangle$, in other words, $A$ is a self-adjoint operator of the Hilbert space $\C^n$.

\begin{theo}
Spectral theorem. A hermitian $n \times n$ matrix has $n$ orthogonal eigenvectors, and their associated eigenvalues are real.
\end{theo}

\begin{proof}
We will prove by induction that any self-adjoint operator of $n$-dimensional Hilbert space has $n$ eigenvectors with associated real eigenvalues. The claim holds trivially in zero-dimensional spaces.

Assume now that the claim holds in $(n-1)$-dimensional Hilbert spaces, and let $A$ be a self-adjoint operator in $n$-dimensional Hilbert space. Consider the characteristic polynomial $\det(A-\lambda I_n)$. This is a polynomial of degree $n\geq 1$, so it has a complex root $\lambda_n$ by the fundamental theorem of algebra.

As $\det(A-\lambda_n I_n)=0$, we can find a vector $v_n \neq 0$ such that $Av_n = \lambda_n v_n$. As we now have an eigenvector, it remains to show that $\lambda_n$ is real and that we can use the induction hypothesis.

Using self-adjointness, we see that
\begin{eqnarray*}
\lambda_n \langle v_n,v_n \rangle & = &
\langle \lambda_n v_n,v_n \rangle\\
& = & \langle Av_n,v_n \rangle \\
& = & \langle v_n,Av_n \rangle \\
& = &\langle v_n,\lambda_n v_n \rangle\\
& = & \overline{\lambda_n} \langle v_n, v_n \rangle.
\end{eqnarray*}

This proves that $\lambda_n$ is real. To use the induction hypothesis, consider a vector $w$ with $\langle v_n,w \rangle = 0$. We see that

\begin{eqnarray*}
\langle v_n, Aw \rangle & = & \langle Av_n , w \rangle\\
& = & \lambda_n \langle v_n,w \rangle\\
& = & 0,
\end{eqnarray*}
so $A$ maps the orthogonal complement of ${tv_n | t\in \C}$ to itself. But this defines a self-adjoint operator of $(n-1)$-dimensional Hilbert space and we may use the induction hypothesis to find its orthogonal eigenvectors $v_1, \ldots, v_{n-1}$ with associated real eigenvalues $\lambda_1, \ldots, \lambda_{n-1}$.
We have found $n$ orthogonal eigenvectors with real eigenvalues, so the claim has been proven.
\end{proof}

\begin{theo}
Min-max theorem of Courant-Fischer-Weyl. For an $n \times n$ Hermitian matrix $A_n$ with eigenvalues $\lambda_1(A_n) \leq \ldots \leq \lambda_n(A_n)$, we have for any $1\leq k \leq n$
\begin{equation*}
\lambda_k(A_n) = \inf \{ \sup\{\langle Ax,x \rangle : x \in V, |x|=1\} : \textrm{ $V$ is $k$-dimensional subspace of $\C^n$} \}
\end{equation*}
and
\begin{equation*}
\lambda_k(A_n) = \sup \{ \inf\{\langle Ax,x \rangle : x \in V\} : \textrm{ $V$ is $(n-k+1)$-dimensional subspace of $\C^n$} \}
\end{equation*}
\end{theo}

\begin{proof}
Let $(v_j)$ be the orthonormal eigenvectors associated with the eigenvalues $(\lambda_j(A_n))$.

Taking $V=span(v_1,\ldots,v_k)$, we see that for any $w=\sum_{i=1}^k c_iv_i$ with $|w|=1$ we have $\langle A_nw,w \rangle = \sum_{i=1}^n \langle \lambda_i(A_n)c_iv_i,c_iv_i \rangle = \sum_{i=1}^k \lambda_i(A_n) \langle c_iv_i,c_iv_i \rangle \leq \lambda_k(A_n) \sum_{i=1}^k \langle c_iv_i,c_iv_i \rangle = \lambda_k(A_n)$. Also, $\langle A_nv_k,v_k \rangle = \lambda_k(A_n)$.

This proves that $\sup \{\langle Ax,x \rangle : x \in V, |x|=1\} = \lambda_k(A_n)$ for this subspace $V$. This means that
\begin{equation*}
\lambda_k(A_n) \geq \inf \{ \sup\{\langle Ax,x \rangle : x \in V, |x|=1\} : \textrm{ $V$ is $k$-dimensional subspace of $\C^n$} \}
\end{equation*}

Let us prove the other direction. For any $k$-dimensional subspace $V$, we see that $V \cap span(v_k,\ldots,v_n) \neq \{0\}$ by considering dimensions. Taking a vector $w=\sum_{i=k}^n c_iv_i$ with $|w|=1$ from this intersection, we get that
$\langle A_nw,w \rangle = \sum_{i=k}^n \lambda_i(A_n) \langle c_iv_i,c_iv_i \rangle \geq \lambda_k(A_n) \sum_{i=k}^n \langle c_iv_i,c_iv_i \rangle = \lambda_k(A_n)$. This means that $\sup\{\langle Ax,x \rangle : x \in V, |x|=1\} \geq \lambda_k(A_n)$ for $V$, and the the first equality follows.

The second equality follows from the first as $\lambda_k(A_n) = -\lambda_{n-k+1}(-A_n)$.
\end{proof}

\begin{theo}
Cauchy interlacing formula. Let $A_n$ be a Hermitian $n \times n$ matrix for $n\geq 2$ and $A_n^l$ be its minor obtained by removing $l$th row and column. Denote the eigenvalues of $A_n$ as $\lambda_1(A_n) \leq \ldots \leq \lambda_n(A_n)$ and the eigenvalues of $A_n^l$ as $\lambda_1(A_n^l) \leq \ldots \leq \lambda_{n-1}(A_n^l)$. Then for any $1 \leq k < n$ we have
\begin{equation*}
\lambda_k(A_n) \leq \lambda_k(A_n^l) \leq \lambda_{k+1}(A_n).
\end{equation*}
\end{theo}

\begin{proof}
It suffices to prove one of the inequalities, as the other follows from the identity $\lambda_k(A_n) = -\lambda_{n-k+1}(-A_n)$.

Define the subspace $U = \{v \in \C_n : v_l=0\} \subset \C^n$, and let $\pi_U$ be the projection $\C^n \to U$, and $\varphi_U$ be the embedding $\C^{n-1} \to U$. We see that $A_n^l = \pi_U \circ A_n \circ \varphi_U$. Let $(v_j)_{j=1}^{n-1}$ be the images of the orthonormal eigenvector of $A_n^l$ under the embedding $\varphi_U$. 

By the previous theorem, we have

\begin{equation*}
\lambda_{k+1}(A_n) = \inf_{V \subset \C^n, \dim V = k+1}  \sup\{\langle A_nx,x \rangle : x \in V, |x|=1\}
\end{equation*}

and, as $\langle \pi_Ux,y \rangle = \langle x,y \rangle$ whenever $y \in U$, we also have

\begin{equation*}
\lambda_k(A_n^l) = \inf \{ \sup\{\langle A_nx,x \rangle : x \in V, |x|=1\} : \textrm{ $V$ is $k$-dimensional subspace of $U$} \}.
\end{equation*}

Let $V$ be any $k+1$-dimensional subspace of $\C^n$. We see that $V \cup span(v_k,\ldots ,v_{n-1}) \neq \{0\}$ by considering dimensions. This means that

\begin{eqnarray*}
\sup\{\langle A_nx,x \rangle : x \in V, |x|=1\} & \geq & \sup\{\langle \pi_UA_nx,\pi_Ux \rangle : x \in V, |x|=1\}\\
& \geq & \lambda_k(A_n^l).
\end{eqnarray*}

Taking the infimum over subspaces $V$ shows that  $\lambda_{k+1}(A_n) \geq \lambda_k(A_n^l)$.

\end{proof}

\chapter{Formulas for the elements of matrix inverse}

\begin{theo}
Cramer's rule. Let $A$ be $n \times n$ invertible square matrix, and for any $1\leq i,j \leq n$, let $M_{ij}$ be the determinant of the $(n-1) \times (n-1)$ matrix obtained from $A$ by removing $i$th row and $j$th column. Then the elements of the inverse matrix $A^{-1}$ are given by
\begin{equation*}
A_{ij}^{-1} = (-1)^{i+j}\frac{M_{ji}}{\det(A)}.
\end{equation*}
\end{theo}

\begin{proof}
Define matrix $B$ by
\begin{equation*}
B_{ij} = (-1)^{i+j}M_{ji}.
\end{equation*}

Fix $1\leq i\leq n$. Now we can compute the diagonal element $(AB)_{ii}$ as

\begin{eqnarray*}
(AB)_{ii} & = & \sum_{k=1}^{n} A_{ik}B_{ki}\\
& = & \sum_{k=1}^{n} (-1)^{i+k} A_{ik}M_{ik}\\
& = & \det(A),
\end{eqnarray*}

where the final equality is obtained by expanding the determinant along the $i$th row.

If we instead have $1\leq j \leq n$, $j\neq i$, we see that
\begin{eqnarray*}
(AB)_{ij} & = & \sum_{k=1}^{n} A_{ik}B_{kj}\\
& = & \sum_{k=1}^{n} (-1)^{j+k} A_{ik}M_{jk}.
\end{eqnarray*}

Now we see that this is the determinant of the $n \times n$ matrix obtained by replacing the $j$th row of $A$ by the $i$th row. But then the rows of the matrix are linearly dependent, so the determinant must be $0$.

We have obtained that $AB = \det(A)I_n$, and as the inverse matrix is unique, we see that $A^{-1} = \frac{1}{\det(A)}B$.
\end{proof}

\begin{theo}
A formula for an element in inverse matrix based on Schur's complement. Let $A_n$ be $n \times n$ invertible square matrix for $n\geq 2$, and denote
\begin{equation*}
A_n =
\begin{pmatrix}
A_{n-1} & C \\
R & a_{nn}
\end{pmatrix} ,
\end{equation*}
where $A_{n-1}$ is $(n-1) \times (n-1)$ matrix, $R$ is $1 \times (n-1)$ row matrix and $C$ $(n-1) \times 1$ column matrix. If both $A_n$ and $A_{n-1}$ are invertible, then we have
\begin{equation*}
[A_n^{-1}]_{nn} = \frac{1}{a_{nn}-R A_{n-1}^{-1} C}.
\end{equation*}
\end{theo}

\begin{proof}
We will solve the linear equation $A_n x = e_n$: the element we seek will be exactly $x_n$. If we define the $n$-vector  $x' = (0\ 0\ \ldots \ 0 \ x_n)^T$, we see that
\begin{equation*}
A_nx' =
\begin{pmatrix}
x_nC \\
x_na_{nn}
\end{pmatrix}.
\end{equation*}

As $A_{n-1}$ is invertible, we may solve the equation $A_{n-1}y = -x_nC$ as $y = -x_nA_{n-1}^{-1}C$. This means that $x = (y \ x_n)^T$, so we get

\begin{equation*}
A_{n}x = 
\begin{pmatrix}
0\\
x_n(a_nn-RA_{n-1}^{-1}C)
\end{pmatrix}.
\end{equation*}

But this should be equal to $e_n$, so we can solve that
\begin{equation*}
x_n = \frac{1}{a_{nn}-RA_{n-1}^{-1}C}.
\end{equation*}
\end{proof}

\begin{thebibliography}{99}

\bibitem{Diac}
Persi Diaconis: Patterns in Eigenvalues: the 70th Josiah Williard Gibbs Lecture, Bull. Amer. Math. Soc. (New Ser.), 40 (2) (2003), p. 155-177.

\bibitem{Wis}
John Wishart: The Generalised Product Moment Distribution in Samples from a Normal Multivariate Population, Biometrika, 20A (1/2) (1928), p. 32-52.

\bibitem{Wig1}
A. Lane, R. G. Thomas, E. P. Wigner: Giant Resonance Interpretation of the Nucleon-Nucleus Interaction, Phys. Rev., 98 (1955), p. 693-701. 

\bibitem{Wig2}
Eugene P. Wigner: Characteristic vectors of bordered matrices with infinite dimensions, Ann. Math., 62 (3) (1955), p. 548-564.

\bibitem{Neu}
J. von Neumann, H. H. Goldstine: Numerical inverting of matrices of high order, Bull. Amer. Math. Soc., 53 (11) (1947), p. 1021-1099.

\bibitem{Hoef}
Wassily Hoeffding: Probability Inequalities for Sums of Bounded Random Variables, Journ. Amer. Stat. Ass., 58 (301) (1963), p. 13-30.

\bibitem{McD}
Colin McDiarmid: On the Method of Bounded Differences, Surveys in Combinatorics, 141 (1989), p. 148-188.

\bibitem{Doob}
Joseph L. Doob: Regularity Properties of Certain Families of Chance Variables, Trans. Amer. Math. Soc., 47 (3) (1940), p. 455-486

\bibitem{Azu}
Kazuoki Azuma: Weighted sums of certain dependent random variables, Tohoku Math. J., 19 (3) (1967), p. 357-367.

\bibitem{Tao}
Terence Tao: Topics in random matrix theory, American Mathematical Society, 2012.

\bibitem{Rud}
Zev Rudnick, Peter Sarnak: Zeroes of principal L-functions and random matrix theory, Duke Math. J., 81 (2) (1996), p. 269-322.

\end{thebibliography}

\end{document}
